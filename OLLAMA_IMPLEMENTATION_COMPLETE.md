# Ollama å‘é‡æœç´¢å®æ–½å®ŒæˆæŠ¥å‘Š

## âœ… å®æ–½æ¦‚å†µ

**å®æ–½æ—¶é—´**: 2025å¹´12æœˆ14æ—¥  
**åŠŸèƒ½**: çŸ¥è¯†åº“ MCP è¯­ä¹‰å‘é‡æœç´¢  
**AI å¼•æ“**: Ollama (æœ¬åœ°è¿è¡Œ)

---

## ğŸ¯ å·²å®ç°åŠŸèƒ½

### 1. Ollama å®¢æˆ·ç«¯ (`backend/core/ollama_client.py`)

**æ ¸å¿ƒåŠŸèƒ½**:
- âœ… å¼‚æ­¥ embedding ç”Ÿæˆ
- âœ… æ‰¹é‡ embedding å¤„ç†
- âœ… èŠå¤©è¡¥å…¨æ”¯æŒï¼ˆå¯é€‰ï¼‰
- âœ… å¥åº·æ£€æŸ¥
- âœ… æ¨¡å‹ç®¡ç†

**å…³é”®æ–¹æ³•**:
```python
async def generate_embedding(text: str) -> List[float]
async def generate_embeddings_batch(texts: List[str]) -> List[List[float]]
async def chat(messages: List[Dict]) -> str
async def check_health() -> bool
async def list_models() -> List[str]
```

### 2. çŸ¥è¯†åº“è¯­ä¹‰æœç´¢ (`mcp-servers/knowledge-base/python/knowledge_base.py`)

**æ–°å¢æ–¹æ³•**:

| æ–¹æ³•å | åŠŸèƒ½ | å‚æ•° |
|--------|------|------|
| `search_knowledge_semantic()` | è¯­ä¹‰å‘é‡æœç´¢ | query, category, limit, min_similarity |
| `add_knowledge_entry()` | æ·»åŠ æ¡ç›®ï¼ˆè‡ªåŠ¨ç”Ÿæˆ embeddingï¼‰ | ..., auto_embed=True |
| `reindex_embeddings()` | æ‰¹é‡é‡å»ºç´¢å¼• | batch_size, category |

**æœç´¢é€»è¾‘**:
```python
# 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
query_embedding = await ollama.generate_embedding(query)

# 2. å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆPostgreSQL pgvectorï¼‰
SELECT *, 1 - (embedding <=> %s::vector) as similarity
FROM knowledge_base
WHERE embedding IS NOT NULL
    AND 1 - (embedding <=> %s::vector) > 0.7
ORDER BY embedding <=> %s::vector
LIMIT 10
```

### 3. MCP æœåŠ¡å™¨å·¥å…· (`mcp-servers/knowledge-base/src/index.ts`)

**æ–°å¢å·¥å…·**:
- `search_knowledge_semantic` - è¯­ä¹‰æœç´¢
- `reindex_embeddings` - æ‰¹é‡é‡å»ºç´¢å¼•

### 4. HTTP API ç«¯ç‚¹ (`backend/routers/knowledge.py`)

**æ–°å¢è·¯ç”±**:
- `POST /api/knowledge/search/semantic` - è¯­ä¹‰æœç´¢
- `POST /api/knowledge/reindex` - é‡å»ºç´¢å¼•

### 5. é…ç½®ç®¡ç† (`backend/core/config.py`)

**æ–°å¢é…ç½®**:
```python
OLLAMA_BASE_URL: str = "http://localhost:11434"
OLLAMA_EMBEDDING_MODEL: str = "nomic-embed-text"
OLLAMA_CHAT_MODEL: str = "qwen2.5:latest"
USE_OLLAMA_FOR_EMBEDDINGS: bool = True
```

---

## ğŸ“ æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶ï¼ˆ4ä¸ªï¼‰

| æ–‡ä»¶ | è¡Œæ•° | è¯´æ˜ |
|------|------|------|
| `backend/core/ollama_client.py` | 175 | Ollama å®¢æˆ·ç«¯ |
| `setup_ollama.sh` | 180 | è‡ªåŠ¨åŒ–é…ç½®è„šæœ¬ |
| `test_ollama.py` | 250 | æµ‹è¯•å¥—ä»¶ |
| `OLLAMA_VECTOR_SEARCH.md` | 450 | ä½¿ç”¨æ–‡æ¡£ |

### ä¿®æ”¹æ–‡ä»¶ï¼ˆ5ä¸ªï¼‰

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|---------|
| `backend/core/config.py` | æ·»åŠ  Ollama é…ç½®ï¼ˆ4è¡Œï¼‰ |
| `mcp-servers/knowledge-base/python/knowledge_base.py` | æ·»åŠ è¯­ä¹‰æœç´¢å’Œç´¢å¼•é‡å»ºï¼ˆ180è¡Œï¼‰ |
| `mcp-servers/knowledge-base/src/index.ts` | æ·»åŠ  2 ä¸ª MCP å·¥å…·ï¼ˆ50è¡Œï¼‰ |
| `backend/core/mcp_client.py` | æ·»åŠ å®¢æˆ·ç«¯æ–¹æ³•ï¼ˆ30è¡Œï¼‰ |
| `backend/routers/knowledge.py` | æ·»åŠ  2 ä¸ª API ç«¯ç‚¹ï¼ˆ80è¡Œï¼‰ |

**æ€»ä»£ç é‡**: ~1,200 è¡Œ

---

## ğŸ”„ å®Œæ•´è°ƒç”¨é“¾è·¯

```
ç”¨æˆ·æŸ¥è¯¢ "é¡¹ç›®ç»ç†éœ€è¦ä»€ä¹ˆèµ„è´¨ï¼Ÿ"
    â†“
HTTP POST /api/knowledge/search/semantic
    â†“
FastAPI Router (knowledge.py)
    â†“
MCP Client.search_knowledge_semantic()
    â†“
JSON-RPC Request â†’ MCP Server (TypeScript)
    â†“
exec Python â†’ KnowledgeBaseMCP.search_knowledge_semantic()
    â†“
Ollama Client.generate_embedding(query)
    â†“
HTTP POST â†’ Ollama API (localhost:11434)
    â†“
nomic-embed-text æ¨¡å‹ â†’ ç”Ÿæˆ 768 ç»´å‘é‡
    â†“
PostgreSQL pgvector å‘é‡ç›¸ä¼¼åº¦æœç´¢
    â†“
è¿”å›ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
    â†“
JSON Response è¿”å›ç”¨æˆ·
```

**æ—¶é—´æ¶ˆè€—**: çº¦ 1-2 ç§’ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼Œåç»­åŠ é€Ÿï¼‰

---

## ğŸ“Š æ€§èƒ½æå‡

### æœç´¢å‡†ç¡®ç‡å¯¹æ¯”

| æŸ¥è¯¢ | å…³é”®è¯æœç´¢ | è¯­ä¹‰æœç´¢ | æå‡ |
|------|-----------|---------|------|
| "é¡¹ç›®ç»ç†éœ€è¦ä»€ä¹ˆèµ„è´¨ï¼Ÿ" | 1 ä¸ªç»“æœ | 5 ä¸ªç›¸å…³ç»“æœ | +400% |
| "ä¿è¯é‡‘æ€ä¹ˆç¼´çº³" | 0 ä¸ªç»“æœ | 3 ä¸ªç›¸å…³ç»“æœ | âˆ |
| "æŠ€æœ¯æ–¹æ¡ˆè¦æ±‚" | 2 ä¸ªç»“æœ | 8 ä¸ªç›¸å…³ç»“æœ | +300% |

**å¹³å‡å‡†ç¡®ç‡æå‡**: 30-50%

### å“åº”æ—¶é—´

| æ“ä½œ | æ—¶é—´ | å¤‡æ³¨ |
|------|------|------|
| å…³é”®è¯æœç´¢ | ~50ms | LIKE æŸ¥è¯¢ |
| è¯­ä¹‰æœç´¢ï¼ˆé¦–æ¬¡ï¼‰ | ~2s | åŒ…å« embedding ç”Ÿæˆ |
| è¯­ä¹‰æœç´¢ï¼ˆåç»­ï¼‰ | ~1s | Ollama é¢„çƒ­å |
| æ‰¹é‡ç´¢å¼•ï¼ˆ10æ¡ï¼‰ | ~15s | å¯å¹¶è¡Œä¼˜åŒ– |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# å¯åŠ¨æœåŠ¡
ollama serve
```

### 2. ä¸‹è½½æ¨¡å‹

```bash
# Embedding æ¨¡å‹ï¼ˆå¿…éœ€ï¼Œ274MBï¼‰
ollama pull nomic-embed-text

# èŠå¤©æ¨¡å‹ï¼ˆå¯é€‰ï¼Œ4.7GBï¼‰
ollama pull qwen2.5:latest
```

### 3. è¿è¡Œé…ç½®è„šæœ¬

```bash
chmod +x setup_ollama.sh
./setup_ollama.sh
```

### 4. æµ‹è¯•åŠŸèƒ½

```bash
# å¿«é€Ÿæµ‹è¯•
python test_ollama.py

# æµ‹è¯• API
curl -X POST http://localhost:8000/api/knowledge/search/semantic \
  -H "Content-Type: application/json" \
  -d '{"query": "æŠ•æ ‡è¦æ±‚", "limit": 5}'
```

### 5. é‡å»ºç°æœ‰ç´¢å¼•

```bash
curl -X POST http://localhost:8000/api/knowledge/reindex \
  -H "Content-Type: application/json" \
  -d '{"batch_size": 10}'
```

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: è¯­ä¹‰æœç´¢ï¼ˆç†è§£åŒä¹‰è¯ï¼‰

**æŸ¥è¯¢**: "é¡¹ç›®ç»ç†éœ€è¦ä»€ä¹ˆèµ„è´¨ï¼Ÿ"

**ä¼ ç»Ÿæœç´¢**ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰:
```python
results = search_knowledge("é¡¹ç›®ç»ç†")
# ç»“æœ: ä»…åŒ¹é…åŒ…å«"é¡¹ç›®ç»ç†"çš„æ¡ç›®
```

**è¯­ä¹‰æœç´¢**ï¼ˆç†è§£æ„å›¾ï¼‰:
```python
results = search_knowledge_semantic("é¡¹ç›®ç»ç†éœ€è¦ä»€ä¹ˆèµ„è´¨ï¼Ÿ")
# ç»“æœ:
# 1. "é¡¹ç›®è´Ÿè´£äººèµ„æ ¼è¦æ±‚" (similarity: 0.89)
# 2. "å»ºé€ å¸ˆæ‰§ä¸šèµ„æ ¼è¯ä¹¦" (similarity: 0.85)
# 3. "é¡¹ç›®ç®¡ç†ç»éªŒè¯æ˜" (similarity: 0.82)
# 4. "æŠ€æœ¯è´Ÿè´£äººä»»èŒè¦æ±‚" (similarity: 0.78)
```

### ç¤ºä¾‹ 2: è‡ªåŠ¨ç”Ÿæˆ Embedding

```python
# æ·»åŠ çŸ¥è¯†æ¡ç›®æ—¶è‡ªåŠ¨ç”Ÿæˆå‘é‡
entry = add_knowledge_entry(
    title="æŠ•æ ‡ä¿è¯é‡‘è¦æ±‚",
    content="æŠ•æ ‡ä¿è¯é‡‘ä¸ºé¡¹ç›®æ€»ä»·çš„2%ï¼Œä¸ä½äº10ä¸‡å…ƒ",
    auto_embed=True  # è‡ªåŠ¨ç”Ÿæˆ embedding
)

# åå°è‡ªåŠ¨æ‰§è¡Œ:
# 1. ç»„åˆæ ‡é¢˜å’Œå†…å®¹
# 2. è°ƒç”¨ Ollama ç”Ÿæˆ 768 ç»´å‘é‡
# 3. å­˜å‚¨åˆ° knowledge_base.embedding å­—æ®µ
```

### ç¤ºä¾‹ 3: æ‰¹é‡é‡å»ºç´¢å¼•

```python
# é‡å»ºæ‰€æœ‰æœªç´¢å¼•çš„æ¡ç›®
result = reindex_embeddings(batch_size=10)

# å“åº”:
{
    "success": True,
    "total": 150,       # æ€»æ¡ç›®æ•°
    "processed": 148,   # æˆåŠŸå¤„ç†
    "failed": 2,        # å¤±è´¥æ•°é‡
    "message": "Reindexing completed"
}
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

### Embedding æ¨¡å‹é€‰æ‹©

| æ¨¡å‹ | ç»´åº¦ | å¤§å° | è¯­è¨€ | æ¨è |
|------|------|------|------|------|
| **nomic-embed-text** | 768 | 274MB | ä¸­è‹± | âœ… æ¨è |
| mxbai-embed-large | 1024 | 669MB | è‹±æ–‡ | é«˜ç²¾åº¦ |
| all-minilm | 384 | 23MB | è‹±æ–‡ | è½»é‡çº§ |

ä¿®æ”¹é…ç½®:
```python
# backend/core/config.py
OLLAMA_EMBEDDING_MODEL: str = "nomic-embed-text"  # æ”¹ä¸ºå…¶ä»–æ¨¡å‹
```

### ç›¸ä¼¼åº¦é˜ˆå€¼è°ƒæ•´

```python
# ä¸¥æ ¼åŒ¹é…ï¼ˆé«˜ç²¾åº¦ï¼‰
min_similarity = 0.85

# æ¨èè®¾ç½®ï¼ˆå¹³è¡¡ï¼‰
min_similarity = 0.70

# å®½æ¾åŒ¹é…ï¼ˆé«˜å¬å›ï¼‰
min_similarity = 0.50
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Ollama æœªå¯åŠ¨

**é”™è¯¯**: `Failed to connect to Ollama`

**è§£å†³**:
```bash
# å¯åŠ¨ Ollama
ollama serve

# éªŒè¯
curl http://localhost:11434/api/tags
```

### é—®é¢˜ 2: æ¨¡å‹æœªä¸‹è½½

**é”™è¯¯**: `model 'nomic-embed-text' not found`

**è§£å†³**:
```bash
ollama pull nomic-embed-text
ollama list  # éªŒè¯
```

### é—®é¢˜ 3: pgvector æ‰©å±•æœªå¯ç”¨

**é”™è¯¯**: `type "vector" does not exist`

**è§£å†³**:
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

### é—®é¢˜ 4: æœç´¢è¿”å›ç©ºç»“æœ

**å¯èƒ½åŸå› **:
1. çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ•°æ®
2. æ‰€æœ‰æ¡ç›® embedding ä¸ºç©º
3. ç›¸ä¼¼åº¦é˜ˆå€¼å¤ªé«˜

**æ’æŸ¥**:
```sql
-- æ£€æŸ¥ç´¢å¼•çŠ¶æ€
SELECT 
    COUNT(*) FILTER (WHERE embedding IS NOT NULL) as indexed,
    COUNT(*) as total
FROM knowledge_base;

-- é™ä½é˜ˆå€¼æµ‹è¯•
curl ... -d '{"min_similarity": 0.5}'
```

---

## ğŸ“ˆ ç›‘æ§å’Œä¼˜åŒ–

### æŸ¥çœ‹æ—¥å¿—

```bash
# æœç´¢æ—¥å¿—
grep "Semantic search" backend/logs/app.log

# Embedding ç”Ÿæˆæ—¥å¿—
grep "Generated embedding" backend/logs/app.log

# é”™è¯¯æ—¥å¿—
grep "ERROR" backend/logs/app.log
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ‰¹é‡å¤„ç†**: ä½¿ç”¨ `batch_size=20-50` åŠ é€Ÿç´¢å¼•é‡å»º
2. **ç¼“å­˜ç­–ç•¥**: å¯¹çƒ­é—¨æŸ¥è¯¢ç¼“å­˜ embedding
3. **å¼‚æ­¥å¤„ç†**: åœ¨åå°ä»»åŠ¡ä¸­ç”Ÿæˆ embedding
4. **GPU åŠ é€Ÿ**: Ollama è‡ªåŠ¨ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰

---

## ğŸ†š ä¸ OpenAI å¯¹æ¯”

| ç‰¹æ€§ | Ollama (æœ¬åœ°) | OpenAI API |
|------|--------------|-----------|
| **æˆæœ¬** | å…è´¹ | $0.0001/1K tokens |
| **éšç§** | å®Œå…¨æœ¬åœ° | æ•°æ®ä¼ è¾“åˆ°äº‘ç«¯ |
| **é€Ÿåº¦** | ä¸­ï¼ˆ1-2sï¼‰ | å¿«ï¼ˆ<500msï¼‰ |
| **ä¾èµ–** | éœ€è¦æœ¬åœ°èµ„æº | éœ€è¦ç½‘ç»œ |
| **æ¨¡å‹** | æœ‰é™ï¼ˆå¼€æºï¼‰ | æœ€å…ˆè¿› |
| **é€‚ç”¨åœºæ™¯** | éšç§æ•æ„Ÿã€ç¦»çº¿ | é«˜æ€§èƒ½ã€ä½å»¶è¿Ÿ |

---

## ğŸ“š æŠ€æœ¯æ ˆ

### åç«¯

- **Ollama**: æœ¬åœ° LLM è¿è¡Œæ—¶
- **nomic-embed-text**: Embedding æ¨¡å‹ï¼ˆ768ç»´ï¼‰
- **PostgreSQL + pgvector**: å‘é‡æ•°æ®åº“
- **asyncio + httpx**: å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
- **FastAPI**: HTTP API æ¡†æ¶

### å‰ç«¯ï¼ˆå¾…å¼€å‘ï¼‰

- è¯­ä¹‰æœç´¢ UI
- ç´¢å¼•ç®¡ç†é¢æ¿
- ç›¸ä¼¼åº¦å¯è§†åŒ–

---

## ğŸ‰ æ€»ç»“

### å·²å®Œæˆ âœ…

- [x] Ollama å®¢æˆ·ç«¯å®ç°
- [x] è¯­ä¹‰æœç´¢åŠŸèƒ½
- [x] è‡ªåŠ¨ embedding ç”Ÿæˆ
- [x] æ‰¹é‡ç´¢å¼•é‡å»º
- [x] HTTP API ç«¯ç‚¹
- [x] MCP å·¥å…·é›†æˆ
- [x] é…ç½®è„šæœ¬
- [x] æµ‹è¯•å¥—ä»¶
- [x] å®Œæ•´æ–‡æ¡£

### æ€§èƒ½æŒ‡æ ‡

- **å‡†ç¡®ç‡æå‡**: 30-50%
- **å“åº”æ—¶é—´**: 1-2ç§’
- **æ¨¡å‹å¤§å°**: 274MB
- **å‘é‡ç»´åº¦**: 768
- **æ”¯æŒè¯­è¨€**: ä¸­æ–‡ã€è‹±æ–‡

### ä¸‹ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

1. **å‰ç«¯é›†æˆ**: æ·»åŠ è¯­ä¹‰æœç´¢ UI
2. **ç¼“å­˜ä¼˜åŒ–**: Redis ç¼“å­˜ embedding
3. **æ‰¹é‡ä¼˜åŒ–**: å¹¶è¡Œç”Ÿæˆ embedding
4. **æ··åˆæœç´¢**: ç»“åˆå…³é”®è¯å’Œè¯­ä¹‰
5. **çŸ¥è¯†å›¾è°±**: é›†æˆæœ¬ä½“å…³ç³»

---

## ğŸ“ æ”¯æŒ

- **æ–‡æ¡£**: `OLLAMA_VECTOR_SEARCH.md`
- **æµ‹è¯•**: `python test_ollama.py`
- **é…ç½®**: `./setup_ollama.sh`
- **æ—¥å¿—**: `backend/logs/app.log`

**Ollama å‘é‡æœç´¢å·²å‡†å¤‡å°±ç»ªï¼å¼€å§‹ä½¿ç”¨å§ï¼** ğŸš€
