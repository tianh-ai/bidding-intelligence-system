# -*- coding: utf-8 -*-
"""
enhanced_database.py - å¢å¼ºçš„æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
åŒ…å«å…³ç³»é€»è¾‘è¿½è¸ªè¡¨ï¼Œç”¨äºè®°å½•verify_new_parserå’Œparsing_resultsçš„å®Œæ•´å…³ç³»
"""

import asyncio
import asyncpg
import os
from datetime import datetime
from loguru import logger

# æ•°æ®åº“è¿æ¥é…ç½®
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = int(os.getenv("DB_PORT", 5432))
DB_NAME = os.getenv("DB_NAME", "bidding_db")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD", "postgres")

logger.add(
    "/Volumes/ssd/bidding-data/logs/enhanced_database.log",
    rotation="500 MB",
    level="INFO"
)


async def get_db_connection():
    """è·å–å¼‚æ­¥æ•°æ®åº“è¿æ¥"""
    return await asyncpg.connect(
        host=DB_HOST,
        port=DB_PORT,
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD
    )


async def create_uploaded_files_table(db):
    """åˆ›å»ºä¸Šä¼ æ–‡ä»¶è¿½è¸ªè¡¨ï¼ˆæ ¸å¿ƒæ¢çº½è¡¨ï¼‰"""
    try:
        query = """
        CREATE TABLE IF NOT EXISTS uploaded_files (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            file_name VARCHAR(255) NOT NULL,
            file_path TEXT NOT NULL,
            file_size BIGINT,
            upload_status TEXT DEFAULT 'pending',  -- pending|completed|failed
            parse_status TEXT DEFAULT 'pending',   -- pending|processing|completed|failed
            storage_location TEXT DEFAULT '/Volumes/ssd/bidding-data/uploads',
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )
        """
        await db.execute(query)
        logger.info("âœ“ uploaded_files è¡¨å·²åˆ›å»º")
        
        # åˆ›å»ºç´¢å¼•
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_uploaded_files_name 
            ON uploaded_files(file_name)
        """)
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_uploaded_files_status 
            ON uploaded_files(upload_status, parse_status)
        """)
        
    except Exception as e:
        logger.warning(f"uploaded_files è¡¨å¯èƒ½å·²å­˜åœ¨: {e}")


async def create_parsing_results_table(db):
    """åˆ›å»ºè§£æç»“æœè¡¨ï¼ˆå­˜å‚¨verify_new_parserçš„è¾“å‡ºï¼‰"""
    try:
        query = """
        CREATE TABLE IF NOT EXISTS parsing_results (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            file_id UUID NOT NULL REFERENCES uploaded_files(id) ON DELETE CASCADE,
            
            -- è§£æåŸºç¡€ä¿¡æ¯
            chapter_count INTEGER,
            parsing_time FLOAT,
            parsing_status TEXT DEFAULT 'pending',  -- pending|completed|failed
            error_message TEXT,
            
            -- å­˜å‚¨å®Œæ•´çš„verify_new_parserç»“æœ
            result_json JSONB DEFAULT NULL,
            
            -- verify_new_parserè¾“å‡ºçš„æ ¸å¿ƒæŒ‡æ ‡
            accuracy_score FLOAT DEFAULT NULL,  -- æˆåŠŸç‡ (87.5)
            matched_toc_items INTEGER DEFAULT NULL,  -- åŒ¹é…é¡¹æ•° (14)
            total_toc_items INTEGER DEFAULT NULL,  -- æ€»TOCé¡¹æ•° (16)
            
            storage_location TEXT DEFAULT '/Volumes/ssd/bidding-data/parsed',
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )
        """
        await db.execute(query)
        logger.info("âœ“ parsing_results è¡¨å·²åˆ›å»º")
        
        # åˆ›å»ºç´¢å¼•
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_parsing_results_file_id 
            ON parsing_results(file_id)
        """)
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_parsing_results_accuracy
            ON parsing_results(accuracy_score)
        """)
        
    except Exception as e:
        logger.warning(f"parsing_results è¡¨å¯èƒ½å·²å­˜åœ¨: {e}")


async def create_knowledge_base_table(db):
    """åˆ›å»ºçŸ¥è¯†åº“è¡¨ï¼ˆä»è§£æç»“æœæå–çš„çŸ¥è¯†æ¡é¡¹ï¼‰"""
    try:
        query = """
        CREATE TABLE IF NOT EXISTS knowledge_base (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            file_id UUID NOT NULL REFERENCES uploaded_files(id) ON DELETE CASCADE,
            
            -- çŸ¥è¯†æ¡é¡¹å†…å®¹
            title VARCHAR(255) NOT NULL,
            content TEXT NOT NULL,
            category VARCHAR(100),
            
            -- æº¯æºä¿¡æ¯
            file_name VARCHAR(255),
            source VARCHAR(100),
            chapter_source VARCHAR(255),  -- æ¥è‡ªå“ªä¸ªç« èŠ‚ (e.g., "ç¬¬ä¸€éƒ¨åˆ†")
            
            -- æå–è´¨é‡æŒ‡æ ‡
            extraction_confidence FLOAT DEFAULT 1.0,  -- 0-1 ç½®ä¿¡åº¦
            
            -- AIå¢å¼º
            embedding vector(1536),
            
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )
        """
        await db.execute(query)
        logger.info("âœ“ knowledge_base è¡¨å·²åˆ›å»º")
        
        # åˆ›å»ºç´¢å¼•
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_knowledge_base_file_id 
            ON knowledge_base(file_id)
        """)
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_knowledge_base_category 
            ON knowledge_base(category)
        """)
        
    except Exception as e:
        logger.warning(f"knowledge_base è¡¨å¯èƒ½å·²å­˜åœ¨: {e}")


async def create_verification_tracking_table(db):
    """åˆ›å»ºéªŒè¯è¿½è¸ªè¡¨ï¼ˆè®°å½•verify_new_parserçš„æ¯ä¸€æ¬¡æ‰§è¡Œï¼‰
    
    è¿™ä¸ªè¡¨ç”¨äºè¿½è¸ªæ¯ä¸ªæ–‡ä»¶çš„éªŒè¯è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - verify_new_parserçš„æ‰§è¡Œç»“æœ
    - ä¸parsing_resultsçš„å…³ç³»
    - éªŒè¯çš„è¯¦ç»†ç»†èŠ‚
    """
    try:
        query = """
        CREATE TABLE IF NOT EXISTS verification_tracking (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            
            -- å…³é”®å…³ç³»
            file_id UUID NOT NULL REFERENCES uploaded_files(id) ON DELETE CASCADE,
            parsing_result_id UUID REFERENCES parsing_results(id) ON DELETE CASCADE,
            
            -- éªŒè¯è¿‡ç¨‹ä¿¡æ¯
            verification_status TEXT DEFAULT 'pending',  -- pending|completed|failed
            verification_start_time TIMESTAMPTZ,
            verification_end_time TIMESTAMPTZ,
            verification_duration_seconds FLOAT,
            
            -- éªŒè¯ç»“æœè¯¦æƒ… (ä»verify_new_parser.py)
            total_toc_items INTEGER NOT NULL,  -- å‚è€ƒTOCé¡¹æ€»æ•° (16)
            matched_toc_items INTEGER NOT NULL,  -- æˆåŠŸåŒ¹é…é¡¹æ•° (14)
            success_rate FLOAT NOT NULL,  -- æˆåŠŸç‡ç™¾åˆ†æ¯” (87.5)
            
            -- ç« èŠ‚æå–ç»Ÿè®¡
            extracted_chapter_count INTEGER,  -- å®é™…æå–çš„ç« èŠ‚æ•° (24)
            
            -- æ¯é¡¹TOCçš„éªŒè¯è¯¦æƒ… (JSONBæ•°ç»„)
            toc_verification_details JSONB DEFAULT NULL,
            
            -- å¤±è´¥é¡¹çš„è¯¦æƒ…
            failed_items JSONB DEFAULT NULL,
            
            -- é”™è¯¯å’Œæ—¥å¿—
            error_message TEXT DEFAULT NULL,
            verification_log TEXT DEFAULT NULL,
            
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )
        """
        await db.execute(query)
        logger.info("âœ“ verification_tracking è¡¨å·²åˆ›å»º")
        
        # åˆ›å»ºç´¢å¼•
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_verification_tracking_file_id 
            ON verification_tracking(file_id)
        """)
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_verification_tracking_parsing_result_id 
            ON verification_tracking(parsing_result_id)
        """)
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_verification_tracking_status 
            ON verification_tracking(verification_status)
        """)
        
    except Exception as e:
        logger.warning(f"verification_tracking è¡¨å¯èƒ½å·²å­˜åœ¨: {e}")


async def create_parsing_to_verification_junction_table(db):
    """åˆ›å»ºparsing_resultsä¸verification_trackingçš„å…³ç³»è¡¨
    
    ç”¨äºè¯¦ç»†è¿½è¸ªä¸¤ä¸ªéªŒè¯è„šæœ¬ä¹‹é—´çš„æ•°æ®æµå’Œä¾èµ–å…³ç³»
    """
    try:
        query = """
        CREATE TABLE IF NOT EXISTS parsing_verification_mapping (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            
            -- å…³é”®å…³ç³»
            file_id UUID NOT NULL REFERENCES uploaded_files(id) ON DELETE CASCADE,
            parsing_result_id UUID NOT NULL REFERENCES parsing_results(id) ON DELETE CASCADE,
            verification_tracking_id UUID NOT NULL REFERENCES verification_tracking(id) ON DELETE CASCADE,
            
            -- æµç¨‹ä¿¡æ¯
            parse_to_verify_delay_seconds FLOAT,  -- è§£æå®Œæˆåˆ°éªŒè¯å¼€å§‹çš„å»¶è¿Ÿ
            data_flow_path TEXT,  -- æ•°æ®æµè·¯å¾„æè¿°
            
            -- éªŒè¯å¯¹æ ‡
            parser_accuracy_vs_verify_score JSONB,  -- å¯¹æ¯”ä¿¡æ¯
            
            -- è´¨é‡è¯„ä¼°
            overall_quality_score FLOAT,  -- 0-1 ç»¼åˆè´¨é‡è¯„åˆ†
            quality_comments TEXT,
            
            created_at TIMESTAMPTZ DEFAULT NOW()
        )
        """
        await db.execute(query)
        logger.info("âœ“ parsing_verification_mapping è¡¨å·²åˆ›å»º")
        
        # åˆ›å»ºç´¢å¼•
        await db.execute("""
            CREATE INDEX IF NOT EXISTS idx_parsing_verification_mapping_file_id 
            ON parsing_verification_mapping(file_id)
        """)
        
    except Exception as e:
        logger.warning(f"parsing_verification_mapping è¡¨å¯èƒ½å·²å­˜åœ¨: {e}")


async def create_relationships_documentation_table(db):
    """åˆ›å»ºå…³ç³»é€»è¾‘æ–‡æ¡£è¡¨
    
    è®°å½•ç³»ç»Ÿä¸­å„è¡¨ä¹‹é—´çš„å…³ç³»å®šä¹‰å’Œæ•°æ®æµè§„åˆ™
    """
    try:
        query = """
        CREATE TABLE IF NOT EXISTS relationships_documentation (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            
            -- å…³ç³»å®šä¹‰
            source_table VARCHAR(255) NOT NULL,
            target_table VARCHAR(255) NOT NULL,
            relationship_type VARCHAR(50) NOT NULL,  -- 1:1, 1:N, N:1, N:N
            
            -- å…³è”å­—æ®µ
            source_field VARCHAR(255),
            target_field VARCHAR(255),
            foreign_key_name VARCHAR(255),
            
            -- æ•°æ®æµä¿¡æ¯
            data_flow_direction VARCHAR(50),  -- source_to_target, bidirectional
            transformation_logic TEXT,  -- æ•°æ®è½¬æ¢é€»è¾‘
            
            -- æ—¶åºå…³ç³»
            execution_order INTEGER,  -- æ‰§è¡Œé¡ºåº
            depends_on TEXT,  -- ä¾èµ–çš„å…¶ä»–å…³ç³»
            
            -- çº¦æŸè§„åˆ™
            cascade_on_delete BOOLEAN DEFAULT FALSE,
            unique_constraint BOOLEAN DEFAULT FALSE,
            
            -- æ–‡æ¡£
            description TEXT,
            examples TEXT,
            notes TEXT,
            
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )
        """
        await db.execute(query)
        logger.info("âœ“ relationships_documentation è¡¨å·²åˆ›å»º")
        
    except Exception as e:
        logger.warning(f"relationships_documentation è¡¨å¯èƒ½å·²å­˜åœ¨: {e}")


async def insert_relationship_documentation(db):
    """æ’å…¥å…³ç³»æ–‡æ¡£æ•°æ®"""
    try:
        relationships = [
            # å…³ç³»1: uploaded_files â†’ parsing_results
            {
                "source_table": "uploaded_files",
                "target_table": "parsing_results",
                "relationship_type": "1:1",
                "source_field": "id",
                "target_field": "file_id",
                "foreign_key_name": "fk_parsing_results_file_id",
                "data_flow_direction": "source_to_target",
                "transformation_logic": "ä¸€ä¸ªä¸Šä¼ çš„æ–‡ä»¶äº§ç”Ÿä¸€æ¡è§£æç»“æœ",
                "execution_order": 1,
                "depends_on": "None",
                "cascade_on_delete": True,
                "description": "æ–‡ä»¶ä¸Šä¼ åè§¦å‘è§£æï¼Œç”Ÿæˆparsing_resultsè®°å½•",
                "examples": "user uploads PDF â†’ parsing_results record created with file_id reference"
            },
            # å…³ç³»2: uploaded_files â†’ knowledge_base
            {
                "source_table": "uploaded_files",
                "target_table": "knowledge_base",
                "relationship_type": "1:N",
                "source_field": "id",
                "target_field": "file_id",
                "foreign_key_name": "fk_knowledge_base_file_id",
                "data_flow_direction": "source_to_target",
                "transformation_logic": "ä¸€ä¸ªæ–‡ä»¶çš„è§£æç»“æœè¢«åˆ†è§£ä¸ºå¤šä¸ªçŸ¥è¯†æ¡é¡¹",
                "execution_order": 3,
                "depends_on": "parsing_results",
                "cascade_on_delete": True,
                "description": "ä»parsing_resultsæå–çŸ¥è¯†æ¡é¡¹ï¼Œå…³è”å›æºæ–‡ä»¶"
            },
            # å…³ç³»3: verify_new_parser â†’ parsing_results
            {
                "source_table": "verify_new_parser",
                "target_table": "parsing_results",
                "relationship_type": "1:1",
                "source_field": "verification_output",
                "target_field": "result_json",
                "data_flow_direction": "source_to_target",
                "transformation_logic": "verifyè„šæœ¬éªŒè¯è¾“å‡ºä¿å­˜ä¸ºJSON",
                "execution_order": 2,
                "depends_on": "uploaded_files â†’ parsing_results",
                "description": "verify_new_parseréªŒè¯ç»“æœç›´æ¥å­˜å…¥parsing_results.result_json",
                "examples": "verify output: {accuracy_score: 87.5, matched: 14, total: 16} â†’ stored in result_json"
            },
            # å…³ç³»4: parsing_results â†’ verification_tracking
            {
                "source_table": "parsing_results",
                "target_table": "verification_tracking",
                "relationship_type": "1:1",
                "source_field": "id",
                "target_field": "parsing_result_id",
                "data_flow_direction": "source_to_target",
                "transformation_logic": "æ¯ä¸ªè§£æç»“æœæœ‰ä¸€æ¡å¯¹åº”çš„éªŒè¯è¿½è¸ªè®°å½•",
                "execution_order": 2,
                "depends_on": "uploaded_files â†’ parsing_results",
                "description": "è®°å½•verify_new_parserçš„æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœ"
            }
        ]
        
        for rel in relationships:
            query = """
            INSERT INTO relationships_documentation 
            (source_table, target_table, relationship_type, source_field, target_field,
             foreign_key_name, data_flow_direction, transformation_logic, execution_order,
             depends_on, cascade_on_delete, description, examples)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
            ON CONFLICT DO NOTHING
            """
            await db.execute(query,
                rel["source_table"],
                rel["target_table"],
                rel["relationship_type"],
                rel.get("source_field"),
                rel.get("target_field"),
                rel.get("foreign_key_name"),
                rel["data_flow_direction"],
                rel["transformation_logic"],
                rel["execution_order"],
                rel.get("depends_on"),
                rel["cascade_on_delete"],
                rel["description"],
                rel.get("examples")
            )
        
        logger.info("âœ“ å…³ç³»æ–‡æ¡£å·²æ’å…¥")
        
    except Exception as e:
        logger.warning(f"æ’å…¥å…³ç³»æ–‡æ¡£å¤±è´¥: {e}")


async def init_database():
    """ä¸»åˆå§‹åŒ–å‡½æ•°"""
    try:
        db = await get_db_connection()
        logger.info("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # Step 1: åˆ›å»ºåŸºç¡€è¡¨ï¼ˆæŒ‰ä¾èµ–é¡ºåºï¼‰
        await create_uploaded_files_table(db)
        await create_parsing_results_table(db)
        await create_knowledge_base_table(db)
        
        # Step 2: åˆ›å»ºå…³ç³»è¿½è¸ªè¡¨
        await create_verification_tracking_table(db)
        await create_parsing_to_verification_junction_table(db)
        
        # Step 3: åˆ›å»ºå…³ç³»æ–‡æ¡£è¡¨
        await create_relationships_documentation_table(db)
        await insert_relationship_documentation(db)
        
        await db.close()
        logger.info("âœ“ æ‰€æœ‰è¡¨åˆ›å»ºå®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âœ— æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


async def verify_storage_paths():
    """éªŒè¯å­˜å‚¨è·¯å¾„"""
    paths = [
        "/Volumes/ssd/bidding-data/uploads",
        "/Volumes/ssd/bidding-data/parsed",
        "/Volumes/ssd/bidding-data/archive",
        "/Volumes/ssd/bidding-data/logs"
    ]
    
    logger.info("éªŒè¯å­˜å‚¨è·¯å¾„:")
    for path in paths:
        if os.path.exists(path):
            logger.info(f"  âœ“ {path}")
        else:
            logger.warning(f"  âš ï¸  {path} ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            os.makedirs(path, exist_ok=True)


async def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ å¢å¼ºæ•°æ®åº“åˆå§‹åŒ– (å«å…³ç³»è¿½è¸ª)")
    print("="*70)
    print()
    
    # éªŒè¯å­˜å‚¨è·¯å¾„
    await verify_storage_paths()
    print()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    success = await init_database()
    
    print()
    print("="*70)
    if success:
        print("âœ… å¢å¼ºæ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")
        print()
        print("æ–°å¢è¡¨:")
        print("  1. verification_tracking - éªŒè¯è¿½è¸ªè¡¨")
        print("  2. parsing_verification_mapping - è§£æ-éªŒè¯æ˜ å°„è¡¨")
        print("  3. relationships_documentation - å…³ç³»æ–‡æ¡£è¡¨")
        print()
        print("æ ¸å¿ƒè¡¨:")
        print("  1. uploaded_files - æ–‡ä»¶è¿½è¸ª (æ ¸å¿ƒæ¢çº½)")
        print("  2. parsing_results - è§£æç»“æœ (verifyè¾“å‡º)")
        print("  3. knowledge_base - çŸ¥è¯†åº“ (æå–å†…å®¹)")
        print()
        print("æ•°æ®æµ:")
        print("  PDFä¸Šä¼  â†’ uploaded_files")
        print("       â†“")
        print("  verify_new_parser.pyæ‰§è¡Œ")
        print("       â†“")
        print("  parsing_results (å«verifyç»“æœ)")
        print("       â†“")
        print("  knowledge_base (å¤šæ¡è®°å½•)")
        print()
        print("å­˜å‚¨é…ç½®:")
        print("  - æ–‡ä»¶ä¸Šä¼ : /Volumes/ssd/bidding-data/uploads")
        print("  - è§£æç»“æœ: /Volumes/ssd/bidding-data/parsed")
        print("  - æ—¥å¿—æ–‡ä»¶: /Volumes/ssd/bidding-data/logs")
        print()
    else:
        print("âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
        return 1
    print("="*70)
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
