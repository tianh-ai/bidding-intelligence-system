#!/usr/bin/env python3
"""
é›†æˆæŒ‡å—: å¦‚ä½•å°†æ–°çš„æ–‡æ¡£å¤„ç†ç³»ç»Ÿé›†æˆåˆ°ç°æœ‰çš„æ–‡ä»¶ä¸Šä¼ è·¯ç”±

è¿™ä¸ªæ–‡ä»¶æä¾›äº†å®Œæ•´çš„é›†æˆæ­¥éª¤å’Œä»£ç ç¤ºä¾‹
"""

# =====================================================
# ç¬¬ 1 æ­¥: åœ¨ routers/files.py ä¸­å¯¼å…¥æ–°æ¨¡å—
# =====================================================

IMPORT_SECTION = """
from fastapi import APIRouter, UploadFile, File, HTTPException
from typing import List
import asyncio
from pathlib import Path
from datetime import datetime

# æ–°å¢å¯¼å…¥
from engines.document_processor import DocumentProcessor
from engines.smart_document_classifier import DocumentType
from db.ontology import OntologyDB  # ç”¨äºå­˜å‚¨åˆ†ç±»ç»“æœ
"""

# =====================================================
# ç¬¬ 2 æ­¥: ä¿®æ”¹ä¸Šä¼ ç«¯ç‚¹
# =====================================================

MODIFIED_UPLOAD_ENDPOINT = """
@router.post("/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    '''
    æ”¹è¿›çš„æ–‡ä»¶ä¸Šä¼ ç«¯ç‚¹ï¼Œé›†æˆæ–‡æ¡£åˆ†ç±»å’Œå¤„ç†
    
    æµç¨‹:
    1. ä¿å­˜æ–‡ä»¶
    2. åˆ†ç±»æ–‡ä»¶ç±»å‹å’Œå¤„ç†ç­–ç•¥
    3. æ ¹æ®ç±»å‹è¿›è¡Œç›¸åº”å¤„ç†
    4. ä¿å­˜åˆ†ç±»ç»“æœåˆ°æ•°æ®åº“
    5. è¿”å›ç»“æœ
    '''
    
    processor = DocumentProcessor()
    ontology_db = OntologyDB()  # è·å–æ•°æ®åº“è¿æ¥
    results = []
    
    try:
        for file in files:
            file_path = None
            file_id = None
            
            try:
                # 1. ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
                upload_dir = Path("backend/uploads")
                upload_dir.mkdir(parents=True, exist_ok=True)
                file_path = upload_dir / file.filename
                
                with open(file_path, 'wb') as f:
                    f.write(await file.read())
                
                logger.info(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {file_path}")
                
                # 2. åœ¨æ•°æ®åº“ä¸­è®°å½•æ–‡ä»¶
                file_id = await ontology_db.save_uploaded_file(
                    filename=file.filename,
                    file_path=str(file_path),
                    file_size=file_path.stat().st_size,
                    upload_timestamp=datetime.now()
                )
                
                # 3. æ–‡æ¡£åˆ†ç±»å’Œå¤„ç†
                logger.info(f"âš™ï¸  å¤„ç†æ–‡ä»¶: {file.filename}")
                processing_result = await processor.process(
                    file_path=str(file_path),
                    filename=file.filename
                )
                
                # 4. ä¿å­˜åˆ†ç±»ç»“æœåˆ°æ•°æ®åº“
                if processing_result['status'] == 'success':
                    classification_id = await ontology_db.save_document_classification(
                        file_id=file_id,
                        classification_data=processing_result['classification'],
                        processing_strategy=processing_result['processing_strategy'],
                        total_pages=processing_result['total_pages']
                    )
                    
                    # 5. æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œåç»­å¤„ç†
                    file_type = processing_result['file_type']
                    
                    if file_type == 'main_proposal':
                        # âœ… ä¸»æ ‡ä¹¦: æå–ç« èŠ‚å¹¶ä¿å­˜åˆ°çŸ¥è¯†åº“
                        chapters = processing_result.get('chapters', [])
                        for chapter in chapters:
                            await ontology_db.save_chapter(
                                file_id=file_id,
                                chapter_data=chapter
                            )
                        logger.info(f"  âœ… æå–å¹¶ä¿å­˜ {len(chapters)} ä¸ªç« èŠ‚")
                    
                    elif file_type == 'financial_report':
                        # ğŸ’¼ è´¢åŠ¡æŠ¥å‘Š: æŒ‰å¹´ä»½åˆ†ç±»ä¿å­˜
                        detected_years = processing_result['classification'].get('financial_years', [])
                        for year in detected_years:
                            year_dir = Path(f"backend/documents/financial_reports/{year}")
                            year_dir.mkdir(parents=True, exist_ok=True)
                            year_file = year_dir / file.filename
                            file_path.rename(year_file)
                            logger.info(f"  ğŸ’¼ è´¢åŠ¡æŠ¥å‘Š {year}: {year_file}")
                    
                    elif file_type in ['license', 'certificate', 'performance_report', 'audit_report']:
                        # ğŸ“„ è¯ä»¶/æŠ¥å‘Š: ä»…ä¿å­˜ï¼Œè®°å½•å…ƒæ•°æ®
                        cert_dir = Path(f"backend/documents/{file_type}s")
                        cert_dir.mkdir(parents=True, exist_ok=True)
                        cert_file = cert_dir / file.filename
                        file_path.rename(cert_file)
                        logger.info(f"  ğŸ“„ {file_type} å·²ä¿å­˜: {cert_file}")
                    
                    elif file_type == 'scan_pdf':
                        # ğŸ” æ‰«æPDF: ä½¿ç”¨ OCR æå–æ–‡æœ¬
                        scan_dir = Path("backend/documents/scans")
                        scan_dir.mkdir(parents=True, exist_ok=True)
                        scan_file = scan_dir / file.filename
                        file_path.rename(scan_file)
                        logger.info(f"  ğŸ” æ‰«ææ–‡ä»¶: {scan_file}")
                    
                    result = {
                        'filename': file.filename,
                        'status': 'success',
                        'file_type': file_type,
                        'classification_id': classification_id,
                        'chapters_count': len(processing_result.get('chapters', [])),
                        'total_pages': processing_result['total_pages'],
                        'message': f'æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_type}'
                    }
                else:
                    result = {
                        'filename': file.filename,
                        'status': 'error',
                        'message': processing_result.get('error', 'å¤„ç†å¤±è´¥'),
                        'error_detail': processing_result.get('error_detail')
                    }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
                results.append({
                    'filename': file.filename,
                    'status': 'error',
                    'message': str(e)
                })
        
        return {
            'status': 'success',
            'message': f'å¤„ç†å®Œæˆ: {len(results)} ä¸ªæ–‡ä»¶',
            'results': results
        }
    
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ å¤„ç†é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
"""

# =====================================================
# ç¬¬ 3 æ­¥: æ·»åŠ æ•°æ®åº“è¾…åŠ©æ–¹æ³•
# =====================================================

DATABASE_HELPER_METHODS = """
# åœ¨ db/ontology.py ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•

class OntologyDB:
    
    async def save_uploaded_file(self, filename: str, file_path: str, 
                                  file_size: int, upload_timestamp):
        '''ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯'''
        query = '''
            INSERT INTO uploaded_files (filename, file_path, file_size, upload_timestamp)
            VALUES ($1, $2, $3, $4)
            RETURNING id
        '''
        return await self.db.fetchval(query, filename, file_path, file_size, upload_timestamp)
    
    async def save_document_classification(self, file_id: int, 
                                          classification_data: dict,
                                          processing_strategy: str,
                                          total_pages: int):
        '''ä¿å­˜æ–‡æ¡£åˆ†ç±»ç»“æœ'''
        query = '''
            INSERT INTO document_classifications 
            (file_id, file_type, processing_strategy, total_pages, 
             text_page_ratio, scan_page_ratio, is_financial_report, 
             is_certificate, detected_years)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
            RETURNING id
        '''
        
        clf = classification_data
        return await self.db.fetchval(
            query,
            file_id,
            clf.get('file_type'),
            processing_strategy,
            total_pages,
            clf.get('text_page_ratio', 0.0),
            clf.get('scan_page_ratio', 0.0),
            clf.get('is_financial_report', False),
            clf.get('is_certificate', False),
            clf.get('detected_years', [])
        )
    
    async def save_chapter(self, file_id: int, chapter_data: dict):
        '''ä¿å­˜æå–çš„ç« èŠ‚'''
        query = '''
            INSERT INTO chapters (file_id, level, title, content)
            VALUES ($1, $2, $3, $4)
            RETURNING id
        '''
        
        return await self.db.fetchval(
            query,
            file_id,
            chapter_data.get('level'),
            chapter_data.get('title'),
            chapter_data.get('content')
        )
"""

# =====================================================
# ç¬¬ 4 æ­¥: æµ‹è¯•é›†æˆ
# =====================================================

INTEGRATION_TEST = """
# test_integrated_upload.py

import asyncio
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_upload_main_proposal():
    '''æµ‹è¯•ä¸Šä¼ ä¸»æ ‡ä¹¦'''
    with open('test_files/æ ‡ä¹¦.pdf', 'rb') as f:
        response = client.post(
            '/api/files/upload',
            files=('file', f)
        )
    
    assert response.status_code == 200
    result = response.json()
    assert result['results'][0]['status'] == 'success'
    assert result['results'][0]['file_type'] == 'main_proposal'
    print(f"âœ… æå–ç« èŠ‚: {result['results'][0]['chapters_count']}")

def test_upload_financial_report():
    '''æµ‹è¯•ä¸Šä¼ è´¢åŠ¡æŠ¥å‘Š'''
    with open('test_files/è´¢åŠ¡æŠ¥å‘Š.pdf', 'rb') as f:
        response = client.post(
            '/api/files/upload',
            files=('file', f)
        )
    
    assert response.status_code == 200
    result = response.json()
    assert result['results'][0]['status'] == 'success'
    assert result['results'][0]['file_type'] == 'financial_report'
    print(f"âœ… æ£€æµ‹å¹´ä»½: {result['results'][0].get('detected_years')}")

def test_upload_certificate():
    '''æµ‹è¯•ä¸Šä¼ è¯ä»¶'''
    with open('test_files/è¥ä¸šæ‰§ç…§.pdf', 'rb') as f:
        response = client.post(
            '/api/files/upload',
            files=('file', f)
        )
    
    assert response.status_code == 200
    result = response.json()
    assert result['results'][0]['status'] == 'success'
    assert result['results'][0]['file_type'] == 'license'
    print("âœ… è¯ä»¶å·²ä¿å­˜ï¼ˆä»…å­˜å‚¨ï¼Œä¸è§£æï¼‰")

if __name__ == '__main__':
    print("ğŸ§ª å¼€å§‹é›†æˆæµ‹è¯•")
    test_upload_main_proposal()
    test_upload_financial_report()
    test_upload_certificate()
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
"""

# =====================================================
# ç¬¬ 5 æ­¥: éƒ¨ç½²æ£€æŸ¥æ¸…å•
# =====================================================

DEPLOYMENT_CHECKLIST = """
éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•:

[ ] 1. æ•°æ®åº“æ¨¡å¼å·²åº”ç”¨
    - è¿è¡Œ: psql -h localhost -d bidding_db -f backend/database/document_processing_schema.sql
    - ç¡®ä¿æ‰€æœ‰è¡¨éƒ½åˆ›å»ºæˆåŠŸ

[ ] 2. ä¾èµ–å·²å®‰è£…
    - è¿è¡Œ: pip install -r backend/requirements.txt
    - ç¡®ä¿ paddlepaddle, paddleocr, pillow å·²å®‰è£…

[ ] 3. å­˜å‚¨ç›®å½•å·²åˆ›å»º
    - backend/uploads/
    - backend/documents/financial_reports/
    - backend/documents/licenses/
    - backend/documents/certificates/
    - backend/documents/performance_reports/
    - backend/documents/scans/

[ ] 4. ç¯å¢ƒå˜é‡å·²é…ç½®
    - .env ä¸­åŒ…å« OPENAI_API_KEY
    - æ•°æ®åº“è¿æ¥å‚æ•°æ­£ç¡®

[ ] 5. æ—¥å¿—å·²é…ç½®
    - backend/logs/ ç›®å½•å­˜åœ¨
    - æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º INFO æˆ– DEBUG

[ ] 6. æµ‹è¯•å·²é€šè¿‡
    - python backend/test_document_processing.py
    - æ‰€æœ‰æµ‹è¯•åº”è¯¥é€šè¿‡

[ ] 7. å‰ç«¯å·²å‡†å¤‡
    - å‰ç«¯èƒ½æ¥æ”¶æ–°çš„ file_type å­—æ®µ
    - UI èƒ½æ˜¾ç¤ºåˆ†ç±»ç»“æœ

[ ] 8. å¤‡ä»½å·²å®Œæˆ
    - å¤‡ä»½ç°æœ‰æ•°æ®åº“
    - å¤‡ä»½ç°æœ‰ä¸Šä¼ ç›®å½•

éƒ¨ç½²å‘½ä»¤:
1. å¯åŠ¨åç«¯: python backend/main.py
2. å¯åŠ¨ Worker: celery -A backend.worker worker
3. å¯åŠ¨å‰ç«¯: npm run dev (å‰ç«¯ç›®å½•)
"""

# =====================================================
# ç¬¬ 6 æ­¥: æ€§èƒ½ç›‘æ§
# =====================================================

PERFORMANCE_MONITORING = """
æ€§èƒ½ç›‘æ§æŸ¥è¯¢:

# æŸ¥çœ‹æœ€è¿‘å¤„ç†çš„æ–‡ä»¶
SELECT 
    f.filename,
    dc.file_type,
    pp.total_time_ms,
    pp.total_pages,
    ROUND(pp.total_time_ms::float / pp.total_pages, 2) as ms_per_page
FROM document_classifications dc
JOIN uploaded_files f ON dc.file_id = f.id
JOIN processing_performance pp ON dc.id = pp.document_classification_id
ORDER BY pp.created_at DESC
LIMIT 10;

# ç»Ÿè®¡å„æ–‡ä»¶ç±»å‹çš„å¤„ç†æ—¶é—´
SELECT 
    file_type,
    COUNT(*) as file_count,
    ROUND(AVG(total_time_ms)) as avg_time_ms,
    MAX(total_time_ms) as max_time_ms,
    ROUND(AVG(total_pages)) as avg_pages
FROM processing_performance
GROUP BY file_type
ORDER BY avg_time_ms DESC;

# æŸ¥çœ‹ OCR ä½¿ç”¨ç‡
SELECT 
    CASE WHEN scan_page_ratio > 0.5 THEN 'high_ocr'
         WHEN scan_page_ratio > 0.2 THEN 'mixed_ocr'
         ELSE 'text_only'
    END as ocr_usage,
    COUNT(*) as file_count,
    ROUND(AVG(scan_page_ratio), 2) as avg_scan_ratio
FROM document_classifications
GROUP BY ocr_usage;

# æ£€æŸ¥æå–æ–¹æ³•çš„å‡†ç¡®ç‡
SELECT 
    extraction_method,
    COUNT(*) as total,
    ROUND(AVG(confidence_score), 3) as avg_confidence,
    COUNT(*) FILTER (WHERE confidence_score > 0.8) as high_confidence
FROM extraction_results
GROUP BY extraction_method;
"""

if __name__ == '__main__':
    print("ğŸ“š é›†æˆæŒ‡å—å·²ç”Ÿæˆ")
    print("è¯·æŒ‰é¡ºåºå‚è€ƒå„ä¸ªéƒ¨åˆ†è¿›è¡Œé›†æˆ")
