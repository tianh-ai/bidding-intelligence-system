"""
æ–‡ä»¶ç®¡ç†è·¯ç”±
æä¾›æ–‡ä»¶ä¸Šä¼ ã€è§£æžã€æŸ¥è¯¢ç­‰åŠŸèƒ½
é‡‡ç”¨ä¸‰é˜¶æ®µæž¶æž„ï¼štemp â†’ parsed â†’ archive
"""
from fastapi import APIRouter, UploadFile, File, HTTPException, Form, BackgroundTasks
from typing import List, Optional
from pydantic import BaseModel
import hashlib
import uuid
import os
import shutil
import json
from datetime import datetime
from engines import ParseEngine
from engines.document_classifier import DocumentClassifier
from database import db
from core.config import get_settings
from core.logger import logger
from core.file_status import FileStatus, DuplicateAction, FileCategory

# router and engines
router = APIRouter()
parse_engine = ParseEngine()
document_classifier = DocumentClassifier()
settings = get_settings()

# ä½¿ç”¨é…ç½®ç³»ç»Ÿä¸­çš„ä¸Šä¼ è·¯å¾„
UPLOAD_DIR = getattr(settings, 'upload_path', os.getenv('UPLOAD_DIR', './uploads'))
TEMP_DIR = os.path.join(UPLOAD_DIR, 'temp')
PARSED_DIR = os.path.join(UPLOAD_DIR, 'parsed')
ARCHIVE_DIR = os.path.join(UPLOAD_DIR, 'archive')

# ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
for directory in [UPLOAD_DIR, TEMP_DIR, PARSED_DIR, ARCHIVE_DIR]:
    os.makedirs(directory, exist_ok=True)

logger.info(f"File upload directories initialized:")
logger.info(f"  - Temp: {TEMP_DIR}")
logger.info(f"  - Parsed: {PARSED_DIR}")
logger.info(f"  - Archive: {ARCHIVE_DIR}")

# ç¡®ä¿uploaded_filesè¡¨å­˜åœ¨å¹¶åŒ…å«sha256åˆ—ï¼ˆå…¼å®¹æ—§schemaï¼‰
try:
    db.execute("""
        CREATE TABLE IF NOT EXISTS uploaded_files (
            id uuid PRIMARY KEY,
            filename text NOT NULL,
            filetype text NOT NULL,
            doc_type text NOT NULL DEFAULT 'other',
            file_path text NOT NULL,
            file_size bigint DEFAULT 0,
            sha256 text DEFAULT NULL,
            created_at timestamptz DEFAULT now()
        )
    """)
    db.execute("CREATE INDEX IF NOT EXISTS idx_uploaded_files_doc_type ON uploaded_files(doc_type)")
    db.execute("CREATE INDEX IF NOT EXISTS idx_uploaded_files_created_at ON uploaded_files(created_at DESC)")

    # å…¼å®¹æ€§è¿ç§»ï¼šä¸ºè¡¨è¡¥é½æœ€æ–°ä»£ç ä¾èµ–çš„åˆ—
    column_migrations = [
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS sha256 text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS status text DEFAULT 'uploaded'",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS uploader text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS temp_path text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS duplicate_action text DEFAULT 'skip'",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS original_file_id uuid",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS version integer DEFAULT 1",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS status_updated_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS parsed_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS archived_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS indexed_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS archive_path text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS category text DEFAULT 'other'",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS semantic_filename text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS metadata jsonb",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS error_log text"
    ]

    for migration_sql in column_migrations:
        try:
            db.execute(migration_sql)
        except Exception as migration_error:
            logger.warning(f"uploaded_filesåˆ—è¿ç§»å¤±è´¥: {migration_sql} - {migration_error}")
except Exception as e:
    print(f"Warning: Could not create or migrate uploaded_files table: {e}")


@router.post("/upload")
async def upload_files(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    uploader: str = Form(...),  # å¿…å¡«ï¼šä¸Šä¼ äºº
    duplicate_action: str = Form(default="skip"),  # overwrite/update/skip
):
    """
    æ‰¹é‡ä¸Šä¼ æ–‡ä»¶ - ä¼˜åŒ–çš„ä¸‰é˜¶æ®µæž¶æž„
    
    é˜¶æ®µ1: ä¸Šä¼ åˆ°ä¸´æ—¶ç›®å½• (temp/)
    é˜¶æ®µ2: åŽå°è§£æžå¹¶å½’æ¡£ (parsed/ â†’ archive/)
    é˜¶æ®µ3: å»ºç«‹çŸ¥è¯†åº“ç´¢å¼•
    
    Args:
        files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨(PDF/Word/Excel/TXT)
        uploader: ä¸Šä¼ äººå§“åï¼ˆå¿…å¡«ï¼‰
        duplicate_action: é‡å¤æ–‡ä»¶å¤„ç†ç­–ç•¥
            - skip: è·³è¿‡é‡å¤æ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰
            - overwrite: è¦†ç›–åŽŸæ–‡ä»¶
            - update: åˆ›å»ºæ–°ç‰ˆæœ¬
    
    Returns:
        {
            status: "success",
            session_id: str,
            uploaded: [{id, name, status, ...}],
            duplicates: [{name, sha256, action, existing_id}],
            failed: [{name, error}]
        }
    """
    logger.info(f"ðŸ“¤ æ”¶åˆ°ä¸Šä¼ è¯·æ±‚ - æ–‡ä»¶æ•°: {len(files)}, ä¸Šä¼ äºº: {uploader}, é‡å¤ç­–ç•¥: {duplicate_action}")
    
    # ç”Ÿæˆsession_idç”¨äºŽæ‰¹é‡ä¸Šä¼ 
    session_id = str(uuid.uuid4())[:8]
    session_temp_dir = os.path.join(TEMP_DIR, session_id)
    os.makedirs(session_temp_dir, exist_ok=True)
    
    uploaded_files = []
    duplicate_files = []
    failed_files = []
    
    for file in files:
        try:
            # 1. éªŒè¯æ–‡ä»¶ç±»åž‹
            if not file.filename.endswith(('.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt')):
                failed_files.append({
                    "name": file.filename,
                    "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"
                })
                continue
            
            # 2. è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
            file_content = await file.read()
            await file.seek(0)
            file_size = len(file_content)
            sha256_hash = hashlib.sha256(file_content).hexdigest()
            
            logger.info(f"  ðŸ“„ å¤„ç†æ–‡ä»¶: {file.filename} (SHA256: {sha256_hash[:16]}...)")
            
            # 3. æ£€æŸ¥é‡å¤æ–‡ä»¶
            try:
                existing = db.query_one(
                    "SELECT * FROM uploaded_files WHERE sha256 = %s AND status != %s",
                    (sha256_hash, FileStatus.DELETED)
                )
            except Exception as e:
                logger.error(f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {e}")
                try:
                    db.conn.rollback()
                except:
                    pass
                existing = None
            
            if existing:
                logger.info(f"  ðŸ” å‘çŽ°é‡å¤æ–‡ä»¶: {file.filename}")
                
                if duplicate_action == "skip":
                    # è·³è¿‡é‡å¤æ–‡ä»¶
                    duplicate_files.append({
                        "name": file.filename,
                        "sha256": sha256_hash,
                        "action": "skipped",
                        "existing_id": existing['id'],
                        "existing_name": existing.get('semantic_filename') or existing['filename'],
                        "message": f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œä¸Šä¼ äºŽ {existing.get('created_at')}"
                    })
                    continue
                
                elif duplicate_action == "overwrite":
                    # è¦†ç›–ï¼šåˆ é™¤æ—§æ–‡ä»¶è®°å½•
                    logger.info(f"  â™»ï¸  è¦†ç›–æ¨¡å¼ï¼šåˆ é™¤æ—§è®°å½• {existing['id']}")
                    old_id = existing['id']
                    try:
                        # åˆ é™¤ç‰©ç†æ–‡ä»¶
                        for path_col in ['temp_path', 'archive_path']:
                            old_path = existing.get(path_col)
                            if old_path and os.path.exists(old_path):
                                os.remove(old_path)
                        
                        # åˆ é™¤æ•°æ®åº“è®°å½•
                        db.execute("DELETE FROM uploaded_files WHERE id = %s", (old_id,))
                        db.execute("DELETE FROM files WHERE id = %s", (old_id,))
                        db.execute("DELETE FROM chapters WHERE file_id = %s", (old_id,))
                        
                    except Exception as e:
                        logger.warning(f"æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")
                
                elif duplicate_action == "update":
                    # æ›´æ–°ï¼šåˆ›å»ºæ–°ç‰ˆæœ¬
                    logger.info(f"  ðŸ“Œ æ›´æ–°æ¨¡å¼ï¼šåˆ›å»ºç‰ˆæœ¬ {existing.get('version', 1) + 1}")
                    # ç»§ç»­å¤„ç†ï¼Œä½†è®°å½•åŽŸæ–‡ä»¶IDå’Œç‰ˆæœ¬å·
            
            # 4. ç”Ÿæˆæ–‡ä»¶IDå¹¶ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            file_id = str(uuid.uuid4())
            file_ext = os.path.splitext(file.filename)[1]
            temp_filename = f"{file_id}{file_ext}"
            temp_path = os.path.join(session_temp_dir, temp_filename)
            
            with open(temp_path, "wb") as buffer:
                buffer.write(file_content)
            
            logger.info(f"  ðŸ’¾ ä¸´æ—¶ä¿å­˜: {temp_path}")
            
            # 5. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆçŠ¶æ€=uploadedï¼‰
            try:
                version = 1
                original_file_id = None
                
                if existing and duplicate_action == "update":
                    version = existing.get('version', 1) + 1
                    original_file_id = existing['id']
                
                db.execute(
                    """
                    INSERT INTO uploaded_files (
                        id, filename, filetype, file_path, file_size, sha256,
                        status, uploader, temp_path, duplicate_action, 
                        original_file_id, version, created_at
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (
                        file_id, file.filename, file_ext[1:], temp_path, file_size, sha256_hash,
                        FileStatus.UPLOADED, uploader, temp_path, duplicate_action,
                        original_file_id, version
                    )
                )
                
                uploaded_files.append({
                    "id": file_id,
                    "name": file.filename,
                    "size": file_size,
                    "status": FileStatus.UPLOADED,
                    "temp_path": temp_path,
                    "uploader": uploader,
                    "version": version,
                    "uploaded_at": datetime.now().isoformat()
                })
                
                logger.info(f"  âœ… æ•°æ®åº“è®°å½•å·²åˆ›å»º: {file_id}")
                
                # 6. æ·»åŠ åŽå°è§£æžä»»åŠ¡
                background_tasks.add_task(
                    parse_and_archive_file,
                    file_id,
                    temp_path,
                    file.filename
                )
                logger.info(f"  âš™ï¸  åŽå°è§£æžä»»åŠ¡å·²è°ƒåº¦")
                
            except Exception as db_error:
                logger.error(f"æ•°æ®åº“å†™å…¥å¤±è´¥: {db_error}")
                try:
                    db.conn.rollback()
                except:
                    pass
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                failed_files.append({
                    "name": file.filename,
                    "error": f"æ•°æ®åº“é”™è¯¯: {str(db_error)}"
                })
                continue
        
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {file.filename} å¤±è´¥: {e}", exc_info=True)
            failed_files.append({
                "name": file.filename,
                "error": str(e)
            })
    
    logger.info(f"ðŸ“Š ä¸Šä¼ å®Œæˆ - æˆåŠŸ: {len(uploaded_files)}, é‡å¤: {len(duplicate_files)}, å¤±è´¥: {len(failed_files)}")
    
    return {
        "status": "success",
        "session_id": session_id,
        "uploaded": uploaded_files,
        "duplicates": duplicate_files,
        "failed": failed_files
    }
    
    # éªŒè¯doc_type
    if doc_type not in ['tender', 'proposal', 'reference', 'other']:
        doc_type = 'other'
    
    uploaded_files = []
    failed_files = []
    duplicate_files = []
    parsed_files = []
    
    for file in files:
        # éªŒè¯æ–‡ä»¶ç±»åž‹
        if not file.filename.endswith(('.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt')):
            failed_files.append({"name": file.filename, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"})
            continue
        
        # è¯»å–å¹¶è®¡ç®— SHA256ï¼ˆç”¨äºŽç¨³å¥åˆ¤é‡ï¼‰
        file_content = await file.read()
        await file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        file_size = len(file_content)
        sha256 = hashlib.sha256(file_content).hexdigest()

        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶ï¼ˆåŸºäºŽ sha256ï¼‰
        try:
            existing = db.query_one(
                "SELECT * FROM uploaded_files WHERE sha256 = %s",
                (sha256,)
            )
        except Exception as e:
            logger.error(f"Database query error for {file.filename}: {e}")
            # å›žæ»šäº‹åŠ¡
            try:
                db.conn.rollback()
            except:
                pass
            failed_files.append({"name": file.filename, "error": f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {str(e)}"})
            continue

        if existing and not overwrite:
            duplicate_files.append({
                "name": file.filename,
                "size": file_size,
                "existing_id": existing['id'],
                "message": f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œä¸Šä¼ äºŽ {existing.get('created_at')}",
                "sha256": sha256
            })
            # å‰ç«¯å¯ä»¥å†³å®šæ˜¯å¦è¦†ç›–ï¼ˆé€šè¿‡å†æ¬¡ä¸Šä¼ å¹¶ä¼ é€’ overwrite=trueï¼‰
            continue

        if existing and overwrite:
            # åˆ é™¤æ—§è®°å½•åŠæ–‡ä»¶ï¼ˆä¿å®ˆåˆ é™¤ï¼šuploaded_files + files + chaptersï¼‰
            try:
                old_id = existing['id']
                old_path = existing.get('file_path')
                db.execute("DELETE FROM uploaded_files WHERE id = %s", (old_id,))
                db.execute("DELETE FROM files WHERE id = %s", (old_id,))
                db.execute("DELETE FROM chapters WHERE file_id = %s", (old_id,))
                if old_path and os.path.exists(old_path):
                    os.remove(old_path)
            except Exception as e:
                logger.warning(f"Failed to remove existing file record: {e}")
        
        # ä¿å­˜æ–‡ä»¶
        file_id = str(uuid.uuid4())
        file_ext = os.path.splitext(file.filename)[1]
        save_path = os.path.join(UPLOAD_DIR, f"{file_id}{file_ext}")
        
        try:
            with open(save_path, "wb") as buffer:
                buffer.write(file_content)

            # ä¿å­˜æ–‡ä»¶è®°å½•åˆ°æ•°æ®åº“ï¼ˆåŒ…å« sha256ï¼‰
            try:
                db.execute(
                    """
                    INSERT INTO uploaded_files (id, filename, filetype, doc_type, file_path, file_size, sha256, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (file_id, file.filename, file_ext[1:], doc_type, save_path, file_size, sha256)
                )

                uploaded_files.append({
                    "id": file_id,
                    "name": file.filename,
                    "type": doc_type,
                    "size": file_size,
                    "path": save_path,
                    "uploadedAt": datetime.now().isoformat()
                })
                
                logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°æ•°æ®åº“: {file.filename}, ID: {file_id}")

                # å°†è§£æžå·¥ä½œäº¤ç»™åŽå°ä»»åŠ¡ï¼ˆéžé˜»å¡žï¼‰
                try:
                    logger.info(f"Scheduling parse task for file: {file.filename}")
                    # å°†è§£æžä»»åŠ¡æ·»åŠ åˆ° background tasks
                    background_tasks.add_task(
                        parse_and_store,
                        file_id,
                        save_path,
                        file.filename,
                        doc_type
                    )

                    parsed_files.append({
                        "id": file_id,
                        "name": file.filename,
                        "status": "parsing_scheduled"
                    })

                except Exception as parse_error:
                    logger.error(f"Failed to schedule parse for {file.filename}: {parse_error}")
                    # è§£æžè°ƒåº¦å¤±è´¥ä¸å½±å“ä¸Šä¼ 

            except Exception as db_error:
                logger.error(f"Database error for file {file.filename}: {db_error}")
                # å›žæ»šäº‹åŠ¡
                try:
                    db.conn.rollback()
                except:
                    pass
                # åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶
                if os.path.exists(save_path):
                    os.remove(save_path)
                failed_files.append({"name": file.filename, "error": f"æ•°æ®åº“é”™è¯¯: {str(db_error)}"})
                continue
                
        except Exception as e:
            logger.error(f"Upload error for file {file.filename}: {e}")
            # åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶
            if os.path.exists(save_path):
                os.remove(save_path)
            failed_files.append({"name": file.filename, "error": str(e)})
    
    return {
        "status": "success",
        "totalFiles": len(uploaded_files),
        "files": uploaded_files,
        "matchedPairs": 0,  # åŽç»­å®žçŽ°æ–‡ä»¶åŒ¹é…é€»è¾‘
        "unmatchedFiles": [f["name"] for f in failed_files],
        "failed": failed_files,
        "duplicates": duplicate_files,  # é‡å¤æ–‡ä»¶åˆ—è¡¨
        "parsed": parsed_files  # è§£æžä»»åŠ¡å·²è°ƒåº¦æˆ–å®Œæˆçš„æ–‡ä»¶åˆ—è¡¨
    }


def parse_and_archive_file(file_id: str, temp_path: str, filename: str):
    """
    åŽå°ä»»åŠ¡ï¼šè§£æžæ–‡ä»¶å¹¶è‡ªåŠ¨å½’æ¡£
    
    æµç¨‹ï¼š
    1. æ›´æ–°çŠ¶æ€ä¸º PARSING
    2. è§£æžæ–‡ä»¶ï¼ˆæå–æ–‡æœ¬ã€è¡¨æ ¼ã€ç« èŠ‚ï¼‰
    3. æ™ºèƒ½åˆ†ç±»ï¼ˆå¿«é€Ÿ/è¯¦ç»†åˆ†æžï¼‰
    4. å½’æ¡£åˆ° archive/{year}/{month}/{category}/
    5. åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    6. å»ºç«‹çŸ¥è¯†åº“ç´¢å¼•
    
    Args:
        file_id: æ–‡ä»¶ID
        temp_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        filename: åŽŸå§‹æ–‡ä»¶å
    """
    try:
        logger.info(f"ðŸ”„ å¼€å§‹è§£æž: {filename}")
        
        # 1. æ›´æ–°çŠ¶æ€ä¸ºPARSING
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s, status_updated_at = NOW() WHERE id = %s",
                (FileStatus.PARSING, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        # 2. è§£æžæ–‡ä»¶
        parsed_result = parse_engine.parse(temp_path, "other")
        content = parsed_result.get('content', '')
        chapters = parsed_result.get('chapters', [])
        
        # æå–å…ƒæ•°æ®
        metadata = {
            "chapters": [
                {
                    "title": ch.get('title', ''),
                    "level": ch.get('level', 1),
                    "page": ch.get('page', 0)
                }
                for ch in chapters
            ],
            "page_count": len(chapters),
            "has_tables": bool(parsed_result.get('tables')),
            "parse_time": datetime.now().isoformat()
        }
        
        logger.info(f"  ðŸ“ è§£æžå®Œæˆ: {len(chapters)} ä¸ªç« èŠ‚")
        
        # 3. æ›´æ–°çŠ¶æ€ä¸ºPARSEDå¹¶ä¿å­˜å…ƒæ•°æ®
        try:
            db.execute(
                """
                UPDATE uploaded_files 
                SET status = %s, parsed_at = NOW(), metadata = %s
                WHERE id = %s
                """,
                (FileStatus.PARSED, json.dumps(metadata), file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        # 4. æ™ºèƒ½åˆ†ç±»
        category, semantic_filename = document_classifier.classify(
            filename,
            metadata,
            content
        )
        
        logger.info(f"  ðŸ·ï¸  åˆ†ç±»: {category}, è¯­ä¹‰å: {semantic_filename}")
        
        # 5. ç”Ÿæˆå½’æ¡£è·¯å¾„
        now = datetime.now()
        year = now.year
        month = now.month
        archive_dir = os.path.join(ARCHIVE_DIR, str(year), f"{month:02d}", category)
        os.makedirs(archive_dir, exist_ok=True)
        
        archive_path = os.path.join(archive_dir, semantic_filename)
        
        # 6. ç§»åŠ¨æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s WHERE id = %s",
                (FileStatus.ARCHIVING, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        shutil.copy2(temp_path, archive_path)
        logger.info(f"  ðŸ“¦ å½’æ¡£åˆ°: {archive_path}")
        
        # 7. æ›´æ–°æ•°æ®åº“
        try:
            db.execute(
                """
                UPDATE uploaded_files 
                SET status = %s, archive_path = %s, category = %s, 
                    semantic_filename = %s, archived_at = NOW(), file_path = %s
                WHERE id = %s
                """,
                (FileStatus.ARCHIVED, archive_path, category, semantic_filename, archive_path, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        # 8. ä¿å­˜åˆ°filesè¡¨ï¼ˆç”¨äºŽçŸ¥è¯†åº“ï¼‰
        try:
            db.execute(
                """
                INSERT INTO files (id, filename, filepath, doc_type, content, created_at)
                VALUES (%s, %s, %s, %s, %s, NOW())
                ON CONFLICT (id) DO UPDATE SET content = EXCLUDED.content
                """,
                (file_id, semantic_filename, archive_path, category, content)
            )
            
            # ä¿å­˜ç« èŠ‚
            for idx, chapter in enumerate(chapters):
                chapter_id = str(uuid.uuid4())
                db.execute(
                    """
                    INSERT INTO chapters (
                        id, file_id, chapter_number, chapter_title, 
                        chapter_level, content, position_order, created_at
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (
                        chapter_id, file_id,
                        chapter.get('chapter_number', str(idx+1)),
                        chapter.get('title', f'ç¬¬{idx+1}ç« '),
                        chapter.get('level', 1),
                        chapter.get('content', ''),
                        idx + 1
                    )
                )
            
            logger.info(f"  ðŸ“š çŸ¥è¯†åº“è®°å½•å·²ä¿å­˜")
            
        except Exception as db_err:
            logger.error(f"ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {db_err}")
            try:
                db.conn.rollback()
            except:
                pass
        
        # 9. åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_path):
                os.remove(temp_path)
                # å°è¯•åˆ é™¤ç©ºçš„sessionç›®å½•
                session_dir = os.path.dirname(temp_path)
                if os.path.isdir(session_dir) and not os.listdir(session_dir):
                    os.rmdir(session_dir)
                logger.info(f"  ðŸ—‘ï¸  ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤")
        except Exception as e:
            logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        # 10. æ›´æ–°çŠ¶æ€ä¸ºINDEXEDï¼ˆç®€åŒ–ç‰ˆï¼Œæš‚æ—¶è·³è¿‡å‘é‡ç´¢å¼•ï¼‰
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s, indexed_at = NOW() WHERE id = %s",
                (FileStatus.INDEXED, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            logger.warning(f"æ›´æ–°ç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
        
        logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {filename} â†’ {semantic_filename}")
        
    except Exception as e:
        logger.error(f"âŒ è§£æžå½’æ¡£å¤±è´¥ {filename}: {e}", exc_info=True)
        # æ›´æ–°çŠ¶æ€ä¸ºPARSE_FAILED
        try:
            db.execute(
                """
                UPDATE uploaded_files 
                SET status = %s, error_log = %s, status_updated_at = NOW() 
                WHERE id = %s
                """,
                (FileStatus.PARSE_FAILED, str(e), file_id)
            )
        except Exception as update_err:
            try:
                db.conn.rollback()
            except:
                pass
            logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€å¤±è´¥: {update_err}")


def parse_and_store(file_id: str, save_path: str, filename: str, doc_type: str):
    """
    åŽå°è§£æžä»»åŠ¡ï¼šè§£æžæ–‡ä»¶å¹¶å†™å…¥ files ä¸Ž chapters è¡¨
    """
    try:
        logger.info(f"Background parse start: {filename}")
        parsed_result = parse_engine.parse(save_path, doc_type)

        # ä¿å­˜è§£æžç»“æžœåˆ° files è¡¨
        try:
            db.execute(
                """
                INSERT INTO files (id, filename, filepath, doc_type, content, created_at)
                VALUES (%s, %s, %s, %s, %s, NOW())
                """,
                (file_id, filename, save_path, doc_type, parsed_result.get('content', ''))
            )

            # ä¿å­˜ç« èŠ‚ç»“æž„
            chapters = parsed_result.get('chapters', [])
            for idx, chapter in enumerate(chapters):
                chapter_id = str(uuid.uuid4())
                db.execute(
                    """
                    INSERT INTO chapters (id, file_id, chapter_number, chapter_title, chapter_level, content, position_order, structure_data, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (
                        chapter_id,
                        file_id,
                        chapter.get('chapter_number', str(idx+1)),
                        chapter.get('chapter_title', chapter.get('title', f'ç¬¬{idx+1}ç« ')),
                        chapter.get('chapter_level', chapter.get('level', 1)),
                        chapter.get('content', ''),
                        idx + 1,
                        json.dumps(chapter.get('structure', {})) if isinstance(chapter.get('structure', {}), dict) else json.dumps({})
                    )
                )

            logger.info(f"Background parse completed: {filename}, chapters={len(chapters)}")
        except Exception as db_err:
            logger.error(f"Error saving parsed result for {filename}: {db_err}")
    except Exception as e:
        logger.error(f"Parse error in background for {filename}: {e}")


@router.get("")
async def get_files(
    doc_type: Optional[str] = None,
    limit: int = 20,
    offset: int = 0
):
    """
    èŽ·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰ç«¯å…¼å®¹è·¯ç”±ï¼‰
    
    Args:
        doc_type: æ–‡æ¡£ç±»åž‹è¿‡æ»¤(å¯é€‰)
        limit: è¿”å›žæ•°é‡é™åˆ¶
        offset: åç§»é‡
    """
    # å…¼å®¹æ—§å­—æ®µå‘½å: doc_type å¯¹åº”æ–°çš„ category
    return await get_file_list(
        category=doc_type,
        limit=limit,
        offset=offset
    )


@router.get("/list")
async def get_file_list(
    status: Optional[str] = None,
    category: Optional[str] = None,
    uploader: Optional[str] = None,
    limit: int = 50,
    offset: int = 0
):
    """
    èŽ·å–æ–‡ä»¶åˆ—è¡¨ - æ”¯æŒå¤šç»´åº¦ç­›é€‰
    
    Args:
        status: çŠ¶æ€è¿‡æ»¤(uploaded/parsing/parsed/archived/indexed/failed)
        category: åˆ†ç±»è¿‡æ»¤(tender/proposal/contract/report/reference/other)
        uploader: ä¸Šä¼ äººè¿‡æ»¤
        limit: è¿”å›žæ•°é‡é™åˆ¶
        offset: åç§»é‡
    """
    try:
        conditions = []
        params = []
        
        if status:
            conditions.append("status = %s")
            params.append(status)
        
        if category:
            conditions.append("category = %s")
            params.append(category)
        
        if uploader:
            conditions.append("uploader = %s")
            params.append(uploader)
        
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        
        query = f"""
            SELECT 
                id, 
                filename as name, 
                semantic_filename,
                filetype as type, 
                category,
                status,
                file_size as size,
                uploader,
                version,
                archive_path,
                created_at as "uploadedAt",
                parsed_at as "parsedAt",
                archived_at as "archivedAt",
                indexed_at as "indexedAt"
            FROM uploaded_files
            WHERE {where_clause}
            ORDER BY created_at DESC
            LIMIT %s OFFSET %s
        """
        params.extend([limit, offset])
        
        files = db.query(query, tuple(params)) or []
        
        # æ ¼å¼åŒ–è¿”å›žæ•°æ®
        formatted_files = []
        for f in files:
            formatted_files.append({
                "id": f.get("id"),
                "name": f.get("name"),
                "semanticName": f.get("semantic_filename"),
                "type": f.get("type", "other"),
                "category": f.get("category"),
                "status": f.get("status"),
                "size": f.get("size", 0),
                "uploader": f.get("uploader"),
                "version": f.get("version", 1),
                "archivePath": f.get("archive_path"),
                "uploadedAt": str(f.get("uploadedAt", "")),
                "parsedAt": str(f.get("parsedAt", "")) if f.get("parsedAt") else None,
                "archivedAt": str(f.get("archivedAt", "")) if f.get("archivedAt") else None,
                "indexedAt": str(f.get("indexedAt", "")) if f.get("indexedAt") else None,
            })
        
        return {
            "status": "success",
            "files": formatted_files,
            "total": len(formatted_files)
        }
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "success",
            "files": [],
            "total": 0
        }


@router.get("/{file_id}")
async def get_file_detail(file_id: str):
    """
    èŽ·å–æ–‡ä»¶è¯¦æƒ…(åŒ…å«ç« èŠ‚)
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    # èŽ·å–æ–‡ä»¶ä¿¡æ¯
    file = db.query_one(
        "SELECT * FROM files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # èŽ·å–ç« èŠ‚åˆ—è¡¨
    chapters = db.query(
        """
        SELECT id, chapter_number, chapter_title, chapter_level, 
               position_order, structure_data
        FROM chapters
        WHERE file_id = %s
        ORDER BY position_order
        """,
        (file_id,)
    )
    
    file['chapters'] = chapters
    return file


@router.get("/{file_id}/chapters")
async def get_chapters(file_id: str):
    """
    èŽ·å–æ–‡ä»¶çš„æ‰€æœ‰ç« èŠ‚
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    chapters = db.query(
        "SELECT * FROM chapters WHERE file_id = %s ORDER BY position_order",
        (file_id,)
    )
    
    return {
        "file_id": file_id,
        "total": len(chapters),
        "chapters": chapters
    }


@router.get("/chapter/{chapter_id}")
async def get_chapter_detail(chapter_id: str):
    """
    èŽ·å–ç« èŠ‚è¯¦æƒ…(åŒ…å«å®Œæ•´å†…å®¹)
    
    Args:
        chapter_id: ç« èŠ‚ID
    """
    chapter = db.query_one(
        "SELECT * FROM chapters WHERE id = %s",
        (chapter_id,)
    )
    
    if not chapter:
        raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
    
    return chapter


@router.delete("/{file_id}")
async def delete_file(file_id: str):
    """
    åˆ é™¤æ–‡ä»¶(åŠå…¶å…³è”çš„ç« èŠ‚)
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file = db.query_one(
        "SELECT * FROM files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ é™¤ç‰©ç†æ–‡ä»¶
    if file['filepath'] and os.path.exists(file['filepath']):
        os.remove(file['filepath'])
    
    # åˆ é™¤æ•°æ®åº“è®°å½•(CASCADEä¼šè‡ªåŠ¨åˆ é™¤å…³è”ç« èŠ‚)
    db.execute("DELETE FROM files WHERE id = %s", (file_id,))
    
    return {"status": "success", "message": "æ–‡ä»¶å·²åˆ é™¤"}


@router.delete("/uploaded/{file_id}")
async def delete_uploaded_file(file_id: str):
    """
    åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆuploaded_filesè¡¨ï¼‰
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file = db.query_one(
        "SELECT * FROM uploaded_files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ é™¤ç‰©ç†æ–‡ä»¶
    if file['file_path'] and os.path.exists(file['file_path']):
        try:
            os.remove(file['file_path'])
        except Exception as e:
            logger.warning(f"Failed to delete physical file: {e}")
    
    # åˆ é™¤æ•°æ®åº“è®°å½•
    db.execute("DELETE FROM uploaded_files WHERE id = %s", (file_id,))
    
    return {"status": "success", "message": "æ–‡ä»¶å·²åˆ é™¤"}


@router.get("/uploaded/{file_id}/download")
async def download_uploaded_file(file_id: str):
    """
    ä¸‹è½½ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆuploaded_filesè¡¨ï¼‰
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    from fastapi.responses import FileResponse
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file = db.query_one(
        "SELECT * FROM uploaded_files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    file_path = file['file_path']
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ç‰©ç†è·¯å¾„ä¸å­˜åœ¨")
    
    return FileResponse(
        path=file_path,
        filename=file['filename'],
        media_type='application/octet-stream'
    )


@router.get("/database-details")
async def get_database_details():
    """
    èŽ·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        {
            totalFiles: int,
            totalSize: int,
            storageUsed: float (MB),
            knowledgeEntries: int,
            lastUpdate: str
        }
    """
    try:
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        total_files_result = db.query_one("SELECT COUNT(*) as count FROM uploaded_files")
        total_files = total_files_result['count'] if total_files_result else 0
        
        # ç»Ÿè®¡æ€»å¤§å°
        total_size_result = db.query_one("SELECT COALESCE(SUM(file_size), 0) as total FROM uploaded_files")
        total_size = total_size_result['total'] if total_size_result else 0
        storage_used_mb = round(total_size / (1024 * 1024), 2)
        
        # ç»Ÿè®¡çŸ¥è¯†åº“æ¡ç›®ï¼ˆå‡è®¾æœ‰knowledge_baseè¡¨ï¼‰
        try:
            kb_result = db.query_one("SELECT COUNT(*) as count FROM knowledge_base")
            kb_count = kb_result['count'] if kb_result else 0
        except:
            kb_count = 0
        
        # èŽ·å–æœ€åŽæ›´æ–°æ—¶é—´
        last_update_result = db.query_one(
            "SELECT MAX(created_at) as last_update FROM uploaded_files"
        )
        last_update = last_update_result['last_update'] if last_update_result and last_update_result['last_update'] else "æœªçŸ¥"
        if last_update != "æœªçŸ¥":
            last_update = str(last_update)
        
        return {
            "totalFiles": total_files,
            "totalSize": total_size,
            "storageUsed": storage_used_mb,
            "knowledgeEntries": kb_count,
            "lastUpdate": last_update
        }
    except Exception as e:
        logger.error(f"Error getting database details: {e}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–æ•°æ®åº“è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.get("/knowledge-base-entries")
async def get_knowledge_base_entries():
    """
    èŽ·å–çŸ¥è¯†åº“æ¡ç›®åˆ—è¡¨
    
    Returns:
        List of knowledge base entries with metadata
    """
    try:
        # æŸ¥è¯¢çŸ¥è¯†åº“è¡¨ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        try:
            entries = db.query_all("""
                SELECT 
                    id,
                    title,
                    content,
                    category,
                    file_name as fileName,
                    created_at as createdAt
                FROM knowledge_base
                ORDER BY created_at DESC
                LIMIT 100
            """)
            
            return [dict(entry) for entry in entries] if entries else []
        except:
            # å¦‚æžœknowledge_baseè¡¨ä¸å­˜åœ¨ï¼Œè¿”å›žç©ºåˆ—è¡¨
            return []
    except Exception as e:
        logger.error(f"Error getting knowledge base entries: {e}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–çŸ¥è¯†åº“æ¡ç›®å¤±è´¥: {str(e)}")


class ProcessFilesRequest(BaseModel):
    fileIds: List[str]


@router.post("/process")
async def process_files(
    background_tasks: BackgroundTasks,
    request: ProcessFilesRequest
):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼šç”ŸæˆçŸ¥è¯†åº“å’Œæ–‡æ¡£ç´¢å¼•
    
    Args:
        fileIds: æ–‡ä»¶IDåˆ—è¡¨
    
    Returns:
        {
            status: str,
            processedFiles: int,
            documentIndexes: List[DocumentIndex]
        }
    """
    file_ids = request.fileIds
    
    if not file_ids:
        raise HTTPException(status_code=400, detail="æœªæä¾›æ–‡ä»¶ID")
    
    try:
        document_indexes = []
        
        for file_id in file_ids:
            # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
            file_info = db.query_one(
                "SELECT * FROM uploaded_files WHERE id = %s",
                (file_id,)
            )
            
            if not file_info:
                logger.warning(f"File not found: {file_id}")
                continue
            
            # è§£æžæ–‡ä»¶ç”Ÿæˆæ–‡æ¡£ç´¢å¼•
            try:
                # è§£æžæ–‡æ¡£ç»“æž„
                parsed_data = parse_engine.parse(file_info['file_path'], file_info['doc_type'])
                
                # ç”Ÿæˆç« èŠ‚ç´¢å¼•
                chapters = parsed_data.get('chapters', [])
                chapter_index = []
                
                for chapter in chapters:
                    chapter_index.append({
                        'title': chapter.get('chapter_title', chapter.get('title', 'æœªå‘½åç« èŠ‚')),
                        'level': chapter.get('chapter_level', chapter.get('level', 1)),
                        'pageNum': chapter.get('page_number', chapter.get('page', 1)),
                        'children': []  # å¯ä»¥é€’å½’å¤„ç†å­ç« èŠ‚
                    })
                
                document_indexes.append({
                    'id': file_id,
                    'fileName': file_info['filename'],
                    'chapters': chapter_index
                })
                
                # æå–çŸ¥è¯†åº“æ¡ç›®ï¼ˆåŽå°ä»»åŠ¡ï¼‰
                background_tasks.add_task(
                    extract_knowledge_entries,
                    file_id,
                    file_info['filename'],
                    parsed_data.get('content', '')
                )
                
            except Exception as parse_error:
                logger.error(f"Error parsing file {file_id}: {parse_error}")
                continue
        
        return {
            "status": "success",
            "processedFiles": len(document_indexes),
            "documentIndexes": document_indexes
        }
        
    except Exception as e:
        logger.error(f"Error processing files: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.get("/document-indexes")
async def get_document_indexes(file_id: Optional[str] = None):
    """
    èŽ·å–æ–‡æ¡£ç´¢å¼•åˆ—è¡¨
    
    Args:
        file_id: å¯é€‰ï¼ŒæŒ‡å®šæ–‡ä»¶ID
    
    Returns:
        List of document indexes
    """
    try:
        # æŸ¥è¯¢æ–‡ä»¶å’Œç« èŠ‚ä¿¡æ¯
        if file_id:
            files = db.query_all(
                "SELECT * FROM files WHERE id = %s",
                (file_id,)
            )
        else:
            files = db.query_all(
                "SELECT * FROM files ORDER BY created_at DESC LIMIT 50"
            )
        
        document_indexes = []
        
        for file in files:
            # æŸ¥è¯¢ç« èŠ‚
            chapters = db.query_all(
                """
                SELECT chapter_number, chapter_title, chapter_level, position_order
                FROM chapters
                WHERE file_id = %s
                ORDER BY position_order
                """,
                (file['id'],)
            )
            
            chapter_index = []
            for chapter in chapters:
                chapter_index.append({
                    'title': chapter['chapter_title'],
                    'level': chapter['chapter_level'],
                    'pageNum': chapter.get('position_order', 1),
                    'children': []
                })
            
            document_indexes.append({
                'id': file['id'],
                'fileName': file['filename'],
                'chapters': chapter_index
            })
        
        return document_indexes
        
    except Exception as e:
        logger.error(f"Error getting document indexes: {e}")
        raise HTTPException(status_code=500, detail=f"èŽ·å–æ–‡æ¡£ç´¢å¼•å¤±è´¥: {str(e)}")


def extract_knowledge_entries(file_id: str, filename: str, content: str):
    """
    åŽå°ä»»åŠ¡ï¼šä»Žæ–‡æ¡£å†…å®¹ä¸­æå–çŸ¥è¯†åº“æ¡ç›®
    """
    try:
        logger.info(f"Extracting knowledge entries from: {filename}")
        
        # è¿™é‡Œå¯ä»¥è°ƒç”¨LLMæˆ–è§„åˆ™å¼•æ“Žæå–çŸ¥è¯†ç‚¹
        # ç®€åŒ–ç‰ˆæœ¬ï¼šæŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 50]
        
        # ç¡®ä¿knowledge_baseè¡¨å­˜åœ¨
        try:
            db.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_base (
                    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
                    title text NOT NULL,
                    content text NOT NULL,
                    category text DEFAULT 'general',
                    file_id uuid,
                    file_name text,
                    created_at timestamptz DEFAULT now()
                )
            """)
        except:
            pass
        
        # æ’å…¥çŸ¥è¯†æ¡ç›®ï¼ˆç¤ºä¾‹ï¼šæå–å‰10ä¸ªæ®µè½ï¼‰
        for idx, para in enumerate(paragraphs[:10]):
            try:
                # ç”Ÿæˆæ ‡é¢˜ï¼ˆå–å‰30ä¸ªå­—ç¬¦ï¼‰
                title = para[:30] + '...' if len(para) > 30 else para
                
                db.execute(
                    """
                    INSERT INTO knowledge_base (title, content, category, file_id, file_name)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (title, para, 'auto-extracted', file_id, filename)
                )
            except Exception as insert_error:
                logger.warning(f"Failed to insert knowledge entry: {insert_error}")
        
        logger.info(f"Knowledge extraction completed for: {filename}")
        
    except Exception as e:
        logger.error(f"Error extracting knowledge entries: {e}")
