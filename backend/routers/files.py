"""
æ–‡ä»¶ç®¡ç†è·¯ç”±
æä¾›æ–‡ä»¶ä¸Šä¼ ã€è§£æã€æŸ¥è¯¢ç­‰åŠŸèƒ½
é‡‡ç”¨ä¸‰é˜¶æ®µæ¶æ„ï¼štemp â†’ parsed â†’ archive
"""
from fastapi import APIRouter, UploadFile, File, HTTPException, Form, BackgroundTasks
from typing import List, Optional
from pydantic import BaseModel
import hashlib
import uuid
import os
import sys
import shutil
import json
from pathlib import Path
from datetime import datetime
from engines import ParseEngine
from engines.document_classifier import DocumentClassifier
from database import db
from core.config import get_settings
from core.logger import logger
from core.file_status import FileStatus, DuplicateAction, FileCategory

# router and engines
router = APIRouter()
parse_engine = ParseEngine()
document_classifier = DocumentClassifier()
settings = get_settings()

# ä½¿ç”¨é…ç½®çš„å­˜å‚¨è·¯å¾„ï¼ˆæ”¯æŒæœ¬åœ°å’Œå®¹å™¨ç¯å¢ƒï¼‰
UPLOAD_DIR = settings.upload_path
TEMP_DIR = os.path.join(UPLOAD_DIR, "temp")
PARSED_DIR = os.path.join(os.path.dirname(UPLOAD_DIR), "parsed")
ARCHIVE_DIR = os.path.join(os.path.dirname(UPLOAD_DIR), "archive")

# ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
for directory in [UPLOAD_DIR, TEMP_DIR, PARSED_DIR, ARCHIVE_DIR]:
    os.makedirs(directory, exist_ok=True)

logger.info(f"File upload directories initialized (SSD Storage):")
logger.info(f"  - Upload: {UPLOAD_DIR}")
logger.info(f"  - Temp: {TEMP_DIR}")
logger.info(f"  - Parsed: {PARSED_DIR}")
logger.info(f"  - Archive: {ARCHIVE_DIR}")
logger.info(f"  - Archive: {ARCHIVE_DIR}")

# ç¡®ä¿uploaded_filesè¡¨å­˜åœ¨å¹¶åŒ…å«sha256åˆ—ï¼ˆå…¼å®¹æ—§schemaï¼‰
try:
    db.execute("""
        CREATE TABLE IF NOT EXISTS uploaded_files (
            id uuid PRIMARY KEY,
            filename text NOT NULL,
            filetype text NOT NULL,
            doc_type text NOT NULL DEFAULT 'other',
            file_path text NOT NULL,
            file_size bigint DEFAULT 0,
            sha256 text DEFAULT NULL,
            created_at timestamptz DEFAULT now()
        )
    """)
    db.execute("CREATE INDEX IF NOT EXISTS idx_uploaded_files_doc_type ON uploaded_files(doc_type)")
    db.execute("CREATE INDEX IF NOT EXISTS idx_uploaded_files_created_at ON uploaded_files(created_at DESC)")

    # å…¼å®¹æ€§è¿ç§»ï¼šä¸ºè¡¨è¡¥é½æœ€æ–°ä»£ç ä¾èµ–çš„åˆ—
    column_migrations = [
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS sha256 text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS status text DEFAULT 'uploaded'",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS uploader text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS temp_path text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS duplicate_action text DEFAULT 'skip'",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS original_file_id uuid",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS version integer DEFAULT 1",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS status_updated_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS parsed_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS archived_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS indexed_at timestamptz",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS archive_path text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS category text DEFAULT 'other'",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS semantic_filename text",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS metadata jsonb",
        "ALTER TABLE uploaded_files ADD COLUMN IF NOT EXISTS error_log text"
    ]

    for migration_sql in column_migrations:
        try:
            db.execute(migration_sql)
        except Exception as migration_error:
            logger.warning(f"uploaded_filesåˆ—è¿ç§»å¤±è´¥: {migration_sql} - {migration_error}")
except Exception as e:
    print(f"Warning: Could not create or migrate uploaded_files table: {e}")


@router.post("/upload")
async def upload_files(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    uploader: str = Form(...),  # å¿…å¡«ï¼šä¸Šä¼ äºº
    duplicate_action: str = Form(default="skip"),  # overwrite/update/skip
):
    """
    æ‰¹é‡ä¸Šä¼ æ–‡ä»¶ - ä¼˜åŒ–çš„ä¸‰é˜¶æ®µæ¶æ„
    
    é˜¶æ®µ1: ä¸Šä¼ åˆ°ä¸´æ—¶ç›®å½• (temp/)
    é˜¶æ®µ2: åå°è§£æå¹¶å½’æ¡£ (parsed/ â†’ archive/)
    é˜¶æ®µ3: å»ºç«‹çŸ¥è¯†åº“ç´¢å¼•
    
    Args:
        files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨(PDF/Word/Excel/TXT)
        uploader: ä¸Šä¼ äººå§“åï¼ˆå¿…å¡«ï¼‰
        duplicate_action: é‡å¤æ–‡ä»¶å¤„ç†ç­–ç•¥
            - skip: è·³è¿‡é‡å¤æ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰
            - overwrite: è¦†ç›–åŸæ–‡ä»¶
            - update: åˆ›å»ºæ–°ç‰ˆæœ¬
    
    Returns:
        {
            status: "success",
            session_id: str,
            uploaded: [{id, name, status, ...}],
            duplicates: [{name, sha256, action, existing_id}],
            failed: [{name, error}]
        }
    """
    logger.info(f"ğŸ“¤ æ”¶åˆ°ä¸Šä¼ è¯·æ±‚ - æ–‡ä»¶æ•°: {len(files)}, ä¸Šä¼ äºº: {uploader}, é‡å¤ç­–ç•¥: {duplicate_action}")
    
    # ç”Ÿæˆsession_idç”¨äºæ‰¹é‡ä¸Šä¼ 
    session_id = str(uuid.uuid4())[:8]
    session_temp_dir = os.path.join(TEMP_DIR, session_id)
    os.makedirs(session_temp_dir, exist_ok=True)
    
    uploaded_files = []
    duplicate_files = []
    failed_files = []
    
    for file in files:
        try:
            # 1. éªŒè¯æ–‡ä»¶ç±»å‹
            if not file.filename.endswith(('.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt')):
                failed_files.append({
                    "name": file.filename,
                    "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"
                })
                continue
            
            # 2. è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
            file_content = await file.read()
            await file.seek(0)
            file_size = len(file_content)
            sha256_hash = hashlib.sha256(file_content).hexdigest()
            
            logger.info(f"  ğŸ“„ å¤„ç†æ–‡ä»¶: {file.filename} (SHA256: {sha256_hash[:16]}...)")
            
            # 3. æ£€æŸ¥é‡å¤æ–‡ä»¶
            try:
                existing = db.query_one(
                    "SELECT * FROM uploaded_files WHERE sha256 = %s AND status != %s",
                    (sha256_hash, FileStatus.DELETED)
                )
            except Exception as e:
                logger.error(f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {e}")
                try:
                    db.conn.rollback()
                except:
                    pass
                existing = None
            
            if existing:
                logger.info(f"  ğŸ” å‘ç°é‡å¤æ–‡ä»¶: {file.filename}")
                
                if duplicate_action == "skip":
                    # è·³è¿‡é‡å¤æ–‡ä»¶
                    duplicate_files.append({
                        "name": file.filename,
                        "sha256": sha256_hash,
                        "action": "skipped",
                        "existing_id": existing['id'],
                        "existing_name": existing.get('semantic_filename') or existing['filename'],
                        "existing_size": existing.get('size', 0),
                        "existing_uploaded_at": existing.get('created_at', '').isoformat() if existing.get('created_at') else None,
                        "message": f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œä¸Šä¼ äº {existing.get('created_at')}"
                    })
                    continue
                
                elif duplicate_action == "overwrite":
                    # è¦†ç›–ï¼šåˆ é™¤æ—§æ–‡ä»¶è®°å½•
                    logger.info(f"  â™»ï¸  è¦†ç›–æ¨¡å¼ï¼šåˆ é™¤æ—§è®°å½• {existing['id']}")
                    old_id = existing['id']
                    try:
                        # åˆ é™¤ç‰©ç†æ–‡ä»¶
                        for path_col in ['temp_path', 'archive_path']:
                            old_path = existing.get(path_col)
                            if old_path and os.path.exists(old_path):
                                os.remove(old_path)
                        
                        # åˆ é™¤æ•°æ®åº“è®°å½•
                        db.execute("DELETE FROM uploaded_files WHERE id = %s", (old_id,))
                        db.execute("DELETE FROM files WHERE id = %s", (old_id,))
                        db.execute("DELETE FROM chapters WHERE file_id = %s", (old_id,))
                        
                    except Exception as e:
                        logger.warning(f"æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")
                
                elif duplicate_action == "update":
                    # æ›´æ–°ï¼šåˆ›å»ºæ–°ç‰ˆæœ¬
                    logger.info(f"  ğŸ“Œ æ›´æ–°æ¨¡å¼ï¼šåˆ›å»ºç‰ˆæœ¬ {existing.get('version', 1) + 1}")
                    # ç»§ç»­å¤„ç†ï¼Œä½†è®°å½•åŸæ–‡ä»¶IDå’Œç‰ˆæœ¬å·
            
            # 4. ç”Ÿæˆæ–‡ä»¶IDå¹¶ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
            file_id = str(uuid.uuid4())
            file_ext = os.path.splitext(file.filename)[1]
            temp_filename = f"{file_id}{file_ext}"
            temp_path = os.path.join(session_temp_dir, temp_filename)
            
            with open(temp_path, "wb") as buffer:
                buffer.write(file_content)
            
            logger.info(f"  ğŸ’¾ ä¸´æ—¶ä¿å­˜: {temp_path}")
            
            # 5. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆçŠ¶æ€=uploadedï¼‰
            try:
                version = 1
                original_file_id = None
                
                if existing and duplicate_action == "update":
                    version = existing.get('version', 1) + 1
                    original_file_id = existing['id']
                
                db.execute(
                    """
                    INSERT INTO uploaded_files (
                        id, filename, filetype, file_path, file_size, sha256,
                        status, uploader, temp_path, duplicate_action, 
                        original_file_id, version, created_at
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (
                        file_id, file.filename, file_ext[1:], temp_path, file_size, sha256_hash,
                        FileStatus.UPLOADED, uploader, temp_path, duplicate_action,
                        original_file_id, version
                    )
                )
                
                uploaded_files.append({
                    "id": file_id,
                    "name": file.filename,
                    "size": file_size,
                    "status": FileStatus.UPLOADED,
                    "temp_path": temp_path,
                    "uploader": uploader,
                    "version": version,
                    "uploaded_at": datetime.now().isoformat()
                })
                
                logger.info(f"  âœ… æ•°æ®åº“è®°å½•å·²åˆ›å»º: {file_id}")
                
                # 6. æ·»åŠ åå°è§£æä»»åŠ¡
                background_tasks.add_task(
                    parse_and_archive_file,
                    file_id,
                    temp_path,
                    file.filename
                )
                logger.info(f"  âš™ï¸  åå°è§£æä»»åŠ¡å·²è°ƒåº¦")
                
            except Exception as db_error:
                logger.error(f"æ•°æ®åº“å†™å…¥å¤±è´¥: {db_error}")
                try:
                    db.conn.rollback()
                except:
                    pass
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                failed_files.append({
                    "name": file.filename,
                    "error": f"æ•°æ®åº“é”™è¯¯: {str(db_error)}"
                })
                continue
        
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {file.filename} å¤±è´¥: {e}", exc_info=True)
            failed_files.append({
                "name": file.filename,
                "error": str(e)
            })
    
    logger.info(f"ğŸ“Š ä¸Šä¼ å®Œæˆ - æˆåŠŸ: {len(uploaded_files)}, é‡å¤: {len(duplicate_files)}, å¤±è´¥: {len(failed_files)}")
    
    return {
        "status": "success",
        "session_id": session_id,
        "uploaded": uploaded_files,
        "duplicates": duplicate_files,
        "failed": failed_files
    }
    
    # éªŒè¯doc_type
    if doc_type not in ['tender', 'proposal', 'reference', 'other']:
        doc_type = 'other'
    
    uploaded_files = []
    failed_files = []
    duplicate_files = []
    parsed_files = []
    
    for file in files:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.filename.endswith(('.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt')):
            failed_files.append({"name": file.filename, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"})
            continue
        
        # è¯»å–å¹¶è®¡ç®— SHA256ï¼ˆç”¨äºç¨³å¥åˆ¤é‡ï¼‰
        file_content = await file.read()
        await file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        file_size = len(file_content)
        sha256 = hashlib.sha256(file_content).hexdigest()

        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶ï¼ˆåŸºäº sha256ï¼‰
        try:
            existing = db.query_one(
                "SELECT * FROM uploaded_files WHERE sha256 = %s",
                (sha256,)
            )
        except Exception as e:
            logger.error(f"Database query error for {file.filename}: {e}")
            # å›æ»šäº‹åŠ¡
            try:
                db.conn.rollback()
            except:
                pass
            failed_files.append({"name": file.filename, "error": f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {str(e)}"})
            continue

        if existing and not overwrite:
            duplicate_files.append({
                "name": file.filename,
                "size": file_size,
                "existing_id": existing['id'],
                "existing_name": existing.get('semantic_filename') or existing['filename'],
                "existing_size": existing.get('size', file_size),
                "existing_uploaded_at": existing.get('created_at', '').isoformat() if existing.get('created_at') else None,
                "message": f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œä¸Šä¼ äº {existing.get('created_at')}",
                "sha256": sha256
            })
            # å‰ç«¯å¯ä»¥å†³å®šæ˜¯å¦è¦†ç›–ï¼ˆé€šè¿‡å†æ¬¡ä¸Šä¼ å¹¶ä¼ é€’ overwrite=trueï¼‰
            continue

        if existing and overwrite:
            # åˆ é™¤æ—§è®°å½•åŠæ–‡ä»¶ï¼ˆä¿å®ˆåˆ é™¤ï¼šuploaded_files + files + chaptersï¼‰
            try:
                old_id = existing['id']
                old_path = existing.get('file_path')
                db.execute("DELETE FROM uploaded_files WHERE id = %s", (old_id,))
                db.execute("DELETE FROM files WHERE id = %s", (old_id,))
                db.execute("DELETE FROM chapters WHERE file_id = %s", (old_id,))
                if old_path and os.path.exists(old_path):
                    os.remove(old_path)
            except Exception as e:
                logger.warning(f"Failed to remove existing file record: {e}")
        
        # ä¿å­˜æ–‡ä»¶
        file_id = str(uuid.uuid4())
        file_ext = os.path.splitext(file.filename)[1]
        save_path = os.path.join(UPLOAD_DIR, f"{file_id}{file_ext}")
        
        try:
            with open(save_path, "wb") as buffer:
                buffer.write(file_content)

            # ä¿å­˜æ–‡ä»¶è®°å½•åˆ°æ•°æ®åº“ï¼ˆåŒ…å« sha256ï¼‰
            try:
                db.execute(
                    """
                    INSERT INTO uploaded_files (id, filename, filetype, doc_type, file_path, file_size, sha256, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (file_id, file.filename, file_ext[1:], doc_type, save_path, file_size, sha256)
                )

                uploaded_files.append({
                    "id": file_id,
                    "name": file.filename,
                    "type": doc_type,
                    "size": file_size,
                    "path": save_path,
                    "uploadedAt": datetime.now().isoformat()
                })
                
                logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°æ•°æ®åº“: {file.filename}, ID: {file_id}")

                # å°†è§£æå·¥ä½œäº¤ç»™åå°ä»»åŠ¡ï¼ˆéé˜»å¡ï¼‰
                try:
                    logger.info(f"Scheduling parse task for file: {file.filename}")
                    # å°†è§£æä»»åŠ¡æ·»åŠ åˆ° background tasks
                    background_tasks.add_task(
                        parse_and_store,
                        file_id,
                        save_path,
                        file.filename,
                        doc_type
                    )

                    parsed_files.append({
                        "id": file_id,
                        "name": file.filename,
                        "status": "parsing_scheduled"
                    })

                except Exception as parse_error:
                    logger.error(f"Failed to schedule parse for {file.filename}: {parse_error}")
                    # è§£æè°ƒåº¦å¤±è´¥ä¸å½±å“ä¸Šä¼ 

            except Exception as db_error:
                logger.error(f"Database error for file {file.filename}: {db_error}")
                # å›æ»šäº‹åŠ¡
                try:
                    db.conn.rollback()
                except:
                    pass
                # åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶
                if os.path.exists(save_path):
                    os.remove(save_path)
                failed_files.append({"name": file.filename, "error": f"æ•°æ®åº“é”™è¯¯: {str(db_error)}"})
                continue
                
        except Exception as e:
            logger.error(f"Upload error for file {file.filename}: {e}")
            # åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶
            if os.path.exists(save_path):
                os.remove(save_path)
            failed_files.append({"name": file.filename, "error": str(e)})
    
    return {
        "status": "success",
        "totalFiles": len(uploaded_files),
        "files": uploaded_files,
        "matchedPairs": 0,  # åç»­å®ç°æ–‡ä»¶åŒ¹é…é€»è¾‘
        "unmatchedFiles": [f["name"] for f in failed_files],
        "failed": failed_files,
        "duplicates": duplicate_files,  # é‡å¤æ–‡ä»¶åˆ—è¡¨
        "parsed": parsed_files  # è§£æä»»åŠ¡å·²è°ƒåº¦æˆ–å®Œæˆçš„æ–‡ä»¶åˆ—è¡¨
    }


def parse_and_archive_file(file_id: str, temp_path: str, filename: str):
    """
    åå°ä»»åŠ¡ï¼šè§£ææ–‡ä»¶å¹¶è‡ªåŠ¨å½’æ¡£
    
    æµç¨‹ï¼š
    1. æ›´æ–°çŠ¶æ€ä¸º PARSING
    2. è§£ææ–‡ä»¶ï¼ˆæå–æ–‡æœ¬ã€è¡¨æ ¼ã€ç« èŠ‚ï¼‰
    3. æ™ºèƒ½åˆ†ç±»ï¼ˆå¿«é€Ÿ/è¯¦ç»†åˆ†æï¼‰
    4. å½’æ¡£åˆ° archive/{year}/{month}/{category}/
    5. åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    6. å»ºç«‹çŸ¥è¯†åº“ç´¢å¼•
    
    Args:
        file_id: æ–‡ä»¶ID
        temp_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        filename: åŸå§‹æ–‡ä»¶å
    """
    try:
        logger.info(f"ğŸ”„ å¼€å§‹è§£æ: {filename}")
        
        # 1. æ›´æ–°çŠ¶æ€ä¸ºPARSING
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s, status_updated_at = NOW() WHERE id = %s",
                (FileStatus.PARSING, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        # 2. è§£ææ–‡ä»¶ï¼ˆä½¿ç”¨å¢å¼ºçš„ç« èŠ‚å†…å®¹æå–å™¨ï¼‰
        allowed_doc_types = {"tender", "proposal", "reference"}
        default_doc_type = "reference"
        
        # ä½¿ç”¨å¢å¼ºçš„è§£æå¼•æ“ï¼ˆåŒ…å«ç« èŠ‚å†…å®¹å’Œæ ¼å¼æå–ï¼‰
        try:
            from engines.chapter_content_extractor import get_chapter_content_extractor
            from engines.format_extractor import get_format_extractor
            
            # å…ˆç”¨ä¼ ç»Ÿæ–¹æ³•æå–åŸºæœ¬å†…å®¹
            parsed_result = parse_engine.parse(temp_path, default_doc_type, save_to_db=False)
            content = parsed_result.get('content', '')
            
            # ä½¿ç”¨å¢å¼ºçš„ç« èŠ‚æå–å™¨è·å–ç« èŠ‚å†…å®¹
            content_extractor = get_chapter_content_extractor(use_ollama=False)
            chapters = content_extractor.extract_chapters_with_content(content)
            
            # å¯¹äºDOCXæ–‡ä»¶ï¼Œæå–æ ¼å¼ä¿¡æ¯
            format_info = {}
            if temp_path.lower().endswith(('.docx', '.doc')):
                try:
                    format_extractor = get_format_extractor()
                    format_info = format_extractor.extract_format_from_docx(temp_path)
                    
                    # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ æ ¼å¼ä¿¡æ¯
                    chapter_formats = format_extractor.extract_chapter_formats(temp_path, chapters)
                    for i, ch in enumerate(chapters):
                        if i < len(chapter_formats):
                            ch['structure_data'] = chapter_formats[i]
                    
                    logger.info(f"  âœ… å¢å¼ºè§£æå™¨: {len(chapters)} ç« èŠ‚, æ ¼å¼ä¿¡æ¯å·²æå–")
                except Exception as fmt_error:
                    logger.warning(f"  âš ï¸  æ ¼å¼æå–å¤±è´¥: {fmt_error}")
            else:
                logger.info(f"  âœ… å¢å¼ºè§£æå™¨: {len(chapters)} ç« èŠ‚ï¼ˆéDOCXï¼Œæ— æ ¼å¼ä¿¡æ¯ï¼‰")
            
        except Exception as parse_error:
            logger.warning(f"  âš ï¸  å¢å¼ºè§£æå™¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿè§£æ: {parse_error}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            parsed_result = parse_engine.parse(temp_path, default_doc_type, save_to_db=False)
            content = parsed_result.get('content', '')
            chapters = parsed_result.get('chapters', [])
            format_info = {}
        
        # æå–å…ƒæ•°æ®
        metadata = {
            "original_filename": filename,  # ä¿å­˜åŸå§‹æ–‡ä»¶å
            "chapters": [
                {
                    "title": ch.get('title', ''),
                    "level": ch.get('level', 1),
                    "page": ch.get('page', 0)
                }
                for ch in chapters
            ],
            "page_count": len(chapters),
            "has_tables": bool(parsed_result.get('tables')),
            "parse_time": datetime.now().isoformat()
        }
        
        logger.info(f"  ğŸ“ è§£æå®Œæˆ: {len(chapters)} ä¸ªç« èŠ‚")
        
        # 3. æ›´æ–°çŠ¶æ€ä¸ºPARSEDå¹¶ä¿å­˜å…ƒæ•°æ®
        try:
            db.execute(
                """
                UPDATE uploaded_files 
                SET status = %s, parsed_at = NOW(), metadata = %s
                WHERE id = %s
                """,
                (FileStatus.PARSED, json.dumps(metadata), file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        # 4. æ™ºèƒ½åˆ†ç±»
        category, semantic_filename = document_classifier.classify(
            filename,
            metadata,
            content
        )

        # ç»Ÿä¸€åˆ†ç±»ï¼Œç¡®ä¿ç¬¦åˆ files.doc_type æ£€æŸ¥çº¦æŸ
        safe_category = category if category in allowed_doc_types else default_doc_type
        
        logger.info(f"  ğŸ·ï¸  åˆ†ç±»: {category}, è¯­ä¹‰å: {semantic_filename}")
        
        # 5. ç”Ÿæˆå½’æ¡£è·¯å¾„
        file_ext = os.path.splitext(filename)[1][1:] if '.' in filename else 'txt'
        now = datetime.now()
        year = now.year
        month = now.month
        archive_dir = os.path.join(ARCHIVE_DIR, str(year), f"{month:02d}", safe_category)
        os.makedirs(archive_dir, exist_ok=True)
        
        archive_path = os.path.join(archive_dir, semantic_filename)
        
        # 6. ç§»åŠ¨æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s WHERE id = %s",
                (FileStatus.ARCHIVING, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        shutil.copy2(temp_path, archive_path)
        logger.info(f"  ğŸ“¦ å½’æ¡£åˆ°: {archive_path}")
        
        # 7. æ›´æ–°æ•°æ®åº“
        try:
            db.execute(
                """
                UPDATE uploaded_files 
                SET status = %s, archive_path = %s, category = %s, 
                    semantic_filename = %s, archived_at = NOW(), file_path = %s
                WHERE id = %s
                """,
                (FileStatus.ARCHIVED, archive_path, safe_category, semantic_filename, archive_path, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            raise e
        
        # 8. ä¿å­˜åˆ°filesè¡¨ï¼ˆç”¨äºçŸ¥è¯†åº“ï¼‰
        try:
            db.execute(
                """
                INSERT INTO files (id, filename, filepath, filetype, doc_type, content, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, NOW())
                ON CONFLICT (id) DO UPDATE SET content = EXCLUDED.content, filetype = EXCLUDED.filetype, doc_type = EXCLUDED.doc_type
                """,
                (file_id, semantic_filename, archive_path, file_ext, safe_category, content)
            )
            
            # ä¿å­˜ç« èŠ‚ï¼ˆåŒ…å«contentå’Œstructure_dataï¼‰
            logger.info(f"  ğŸ“Š ç« èŠ‚æ•°æ®æ ·ä¾‹: {chapters[:2] if chapters else 'æ— '}")
            for idx, chapter in enumerate(chapters):
                chapter_id = str(uuid.uuid4())
                
                # æ¸…ç†æ ‡é¢˜ï¼šå»é™¤ç›®å½•ç‚¹å·å’Œé¡µç 
                raw_title = chapter.get('chapter_title', chapter.get('title', f'ç¬¬{idx+1}ç« '))
                # å»é™¤"...æ•°å­—"æ ¼å¼çš„é¡µç æ ‡è®°
                import re
                clean_title = re.sub(r'[\.ã€‚\s]+\d+$', '', raw_title)  # å»é™¤å°¾éƒ¨çš„ "...123"
                clean_title = re.sub(r'[\.ã€‚]{3,}', '', clean_title)  # å»é™¤è¿ç»­ç‚¹å·
                clean_title = clean_title.strip()
                
                # è·å–ç« èŠ‚å†…å®¹å’Œæ ¼å¼ä¿¡æ¯
                chapter_content = chapter.get('content', '')
                structure_data = chapter.get('structure_data', {})
                
                # å¦‚æœstructure_dataæ˜¯dictï¼Œè½¬ä¸ºJSON
                if isinstance(structure_data, dict):
                    structure_data_json = json.dumps(structure_data, ensure_ascii=False)
                else:
                    structure_data_json = '{}'
                
                db.execute(
                    """
                    INSERT INTO chapters (
                        id, file_id, chapter_number, chapter_title, 
                        chapter_level, content, position_order, structure_data, created_at
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s::jsonb, NOW())
                    """,
                    (
                        chapter_id, file_id,
                        chapter.get('chapter_number', str(idx+1)),
                        clean_title,
                        chapter.get('chapter_level', chapter.get('level', 1)),
                        chapter_content,  # ç°åœ¨æœ‰å†…å®¹äº†ï¼
                        idx + 1,
                        structure_data_json  # æ ¼å¼ä¿¡æ¯
                    )
                )
                
            logger.info(f"  ğŸ“š çŸ¥è¯†åº“è®°å½•å·²ä¿å­˜ï¼ˆåŒ…å«å†…å®¹å’Œæ ¼å¼ä¿¡æ¯ï¼‰")
            
            logger.info(f"  ğŸ“š çŸ¥è¯†åº“è®°å½•å·²ä¿å­˜")
            
        except Exception as db_err:
            logger.error(f"ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {db_err}")
            try:
                db.conn.rollback()
            except:
                pass
        
        # 9. åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_path):
                os.remove(temp_path)
                # å°è¯•åˆ é™¤ç©ºçš„sessionç›®å½•
                session_dir = os.path.dirname(temp_path)
                if os.path.isdir(session_dir) and not os.listdir(session_dir):
                    os.rmdir(session_dir)
                logger.info(f"  ğŸ—‘ï¸  ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤")
        except Exception as e:
            logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        # 10. å‘é‡åŒ–å¹¶å»ºç«‹çŸ¥è¯†åº“ç´¢å¼•
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s WHERE id = %s",
                (FileStatus.INDEXING, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            logger.warning(f"æ›´æ–°ç´¢å¼•ä¸­çŠ¶æ€å¤±è´¥: {e}")
        
        # æå–çŸ¥è¯†åº“æ¡ç›®å¹¶å‘é‡åŒ–
        try:
            # ç®€åŒ–ç‰ˆï¼šæŒ‰ç« èŠ‚å»ºç«‹å‘é‡ç´¢å¼•
            from openai import OpenAI
            openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            for idx, chapter in enumerate(chapters):
                chapter_content = chapter.get('content', '')
                if not chapter_content or len(chapter_content) < 50:
                    continue
                
                # åˆ†å—ï¼ˆæ¯1000å­—ç¬¦ä¸€å—ï¼‰
                chunks = [chapter_content[i:i+1000] for i in range(0, len(chapter_content), 1000)]
                
                for chunk_idx, chunk in enumerate(chunks[:5]):  # é™åˆ¶æ¯ç« æœ€å¤š5å—
                    try:
                        # ç”Ÿæˆå‘é‡
                        response = openai_client.embeddings.create(
                            model="text-embedding-3-small",
                            input=chunk
                        )
                        embedding = response.data[0].embedding
                        
                        # ä¿å­˜åˆ°vectorsè¡¨
                        vector_id = str(uuid.uuid4())
                        db.execute(
                            """
                            INSERT INTO vectors (id, file_id, chunk_type, chunk, embedding, created_at)
                            VALUES (%s, %s, %s, %s, %s, NOW())
                            """,
                            (vector_id, file_id, 'chapter', chunk, embedding)
                        )
                        
                        logger.info(f"  ğŸ” å‘é‡ç´¢å¼•å·²å»ºç«‹: ç« èŠ‚{idx+1} å—{chunk_idx+1}")
                    except Exception as vec_err:
                        logger.warning(f"å‘é‡åŒ–å¤±è´¥: {vec_err}")
                        continue
            
            # æå–çŸ¥è¯†åº“æ¡ç›®
            extract_knowledge_entries(file_id, semantic_filename, content)
            
        except Exception as index_err:
            logger.warning(f"å‘é‡ç´¢å¼•å»ºç«‹å¤±è´¥: {index_err}")
        
        # 11. æ›´æ–°çŠ¶æ€ä¸ºINDEXED
        try:
            db.execute(
                "UPDATE uploaded_files SET status = %s, indexed_at = NOW() WHERE id = %s",
                (FileStatus.INDEXED, file_id)
            )
        except Exception as e:
            try:
                db.conn.rollback()
            except:
                pass
            logger.warning(f"æ›´æ–°ç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
        
        logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {filename} â†’ {semantic_filename}")
        
    except Exception as e:
        logger.error(f"âŒ è§£æå½’æ¡£å¤±è´¥ {filename}: {e}", exc_info=True)
        # æ›´æ–°çŠ¶æ€ä¸ºPARSE_FAILED
        try:
            db.execute(
                """
                UPDATE uploaded_files 
                SET status = %s, error_log = %s, status_updated_at = NOW() 
                WHERE id = %s
                """,
                (FileStatus.PARSE_FAILED, str(e), file_id)
            )
        except Exception as update_err:
            try:
                db.conn.rollback()
            except:
                pass
            logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€å¤±è´¥: {update_err}")


def parse_and_store(file_id: str, save_path: str, filename: str, doc_type: str):
    """
    åå°è§£æä»»åŠ¡ï¼šè§£ææ–‡ä»¶å¹¶å†™å…¥ files ä¸ chapters è¡¨
    """
    try:
        logger.info(f"Background parse start: {filename}")
        # ä¼ é€’ file_id ä»¥å¯ç”¨å›¾ç‰‡æå–
        parsed_result = parse_engine.parse(save_path, doc_type, file_id=file_id)

        # ä¿å­˜è§£æç»“æœåˆ° files è¡¨
        try:
            db.execute(
                """
                INSERT INTO files (id, filename, filepath, doc_type, content, created_at)
                VALUES (%s, %s, %s, %s, %s, NOW())
                """,
                (file_id, filename, save_path, doc_type, parsed_result.get('content', ''))
            )

            # ä¿å­˜ç« èŠ‚ç»“æ„
            chapters = parsed_result.get('chapters', [])
            for idx, chapter in enumerate(chapters):
                chapter_id = str(uuid.uuid4())
                db.execute(
                    """
                    INSERT INTO chapters (id, file_id, chapter_number, chapter_title, chapter_level, content, position_order, structure_data, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
                    """,
                    (
                        chapter_id,
                        file_id,
                        chapter.get('chapter_number', str(idx+1)),
                        chapter.get('chapter_title', chapter.get('title', f'ç¬¬{idx+1}ç« ')),
                        chapter.get('chapter_level', chapter.get('level', 1)),
                        chapter.get('content', ''),
                        idx + 1,
                        json.dumps(chapter.get('structure', {})) if isinstance(chapter.get('structure', {}), dict) else json.dumps({})
                    )
                )

            logger.info(f"Background parse completed: {filename}, chapters={len(chapters)}")
        except Exception as db_err:
            logger.error(f"Error saving parsed result for {filename}: {db_err}")
    except Exception as e:
        logger.error(f"Parse error in background for {filename}: {e}")


@router.get("")
async def get_files(
    doc_type: Optional[str] = None,
    limit: int = 20,
    offset: int = 0
):
    """
    è·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰ç«¯å…¼å®¹è·¯ç”±ï¼‰
    
    Args:
        doc_type: æ–‡æ¡£ç±»å‹è¿‡æ»¤(å¯é€‰)
        limit: è¿”å›æ•°é‡é™åˆ¶
        offset: åç§»é‡
    """
    # å…¼å®¹æ—§å­—æ®µå‘½å: doc_type å¯¹åº”æ–°çš„ category
    return await get_file_list(
        category=doc_type,
        limit=limit,
        offset=offset
    )


@router.get("/list")
async def get_file_list(
    status: Optional[str] = None,
    category: Optional[str] = None,
    uploader: Optional[str] = None,
    limit: int = 50,
    offset: int = 0
):
    """
    è·å–æ–‡ä»¶åˆ—è¡¨ - æ”¯æŒå¤šç»´åº¦ç­›é€‰
    
    Args:
        status: çŠ¶æ€è¿‡æ»¤(uploaded/parsing/parsed/archived/indexed/failed)
        category: åˆ†ç±»è¿‡æ»¤(tender/proposal/contract/report/reference/other)
        uploader: ä¸Šä¼ äººè¿‡æ»¤
        limit: è¿”å›æ•°é‡é™åˆ¶
        offset: åç§»é‡
    """
    try:
        conditions = []
        params = []
        
        if status:
            conditions.append("status = %s")
            params.append(status)
        
        if category:
            conditions.append("category = %s")
            params.append(category)
        
        if uploader:
            conditions.append("uploader = %s")
            params.append(uploader)
        
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        
        query = f"""
            SELECT 
                id, 
                filename as name, 
                semantic_filename,
                filetype as type, 
                category,
                status,
                file_size as size,
                uploader,
                version,
                archive_path,
                created_at as "uploadedAt",
                parsed_at as "parsedAt",
                archived_at as "archivedAt",
                indexed_at as "indexedAt"
            FROM uploaded_files
            WHERE {where_clause}
            ORDER BY created_at DESC
            LIMIT %s OFFSET %s
        """
        params.extend([limit, offset])
        
        files = db.query(query, tuple(params)) or []
        
        # æ ¼å¼åŒ–è¿”å›æ•°æ®
        formatted_files = []
        for f in files:
            formatted_files.append({
                "id": f.get("id"),
                "name": f.get("name"),
                "semanticName": f.get("semantic_filename"),
                "type": f.get("type", "other"),
                "category": f.get("category"),
                "status": f.get("status"),
                "size": f.get("size", 0),
                "uploader": f.get("uploader"),
                "version": f.get("version", 1),
                "archivePath": f.get("archive_path"),
                "uploadedAt": str(f.get("uploadedAt", "")),
                "parsedAt": str(f.get("parsedAt", "")) if f.get("parsedAt") else None,
                "archivedAt": str(f.get("archivedAt", "")) if f.get("archivedAt") else None,
                "indexedAt": str(f.get("indexedAt", "")) if f.get("indexedAt") else None,
            })
        
        return {
            "status": "success",
            "files": formatted_files,
            "total": len(formatted_files)
        }
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "success",
            "files": [],
            "total": 0
        }


@router.get("/stats")
async def get_file_stats():
    """
    è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        {
            total_files: int,
            total_size: int,
            by_category: {...},
            by_status: {...},
            recent_uploads: [...]
        }
    """
    try:
        # æ€»æ–‡ä»¶æ•°å’Œæ€»å¤§å°
        stats_query = """
            SELECT 
                COUNT(*) as total_files,
                COALESCE(SUM(file_size), 0) as total_size
            FROM uploaded_files
        """
        stats = db.query_one(stats_query) or {"total_files": 0, "total_size": 0}
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = db.query("""
            SELECT category, COUNT(*) as count
            FROM uploaded_files
            GROUP BY category
        """) or []
        
        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        status_stats = db.query("""
            SELECT status, COUNT(*) as count
            FROM uploaded_files
            GROUP BY status
        """) or []
        
        # æœ€è¿‘ä¸Šä¼ 
        recent_uploads = db.query("""
            SELECT filename, category, file_size, created_at
            FROM uploaded_files
            ORDER BY created_at DESC
            LIMIT 5
        """) or []
        
        return {
            "status": "success",
            "total_files": stats.get("total_files", 0),
            "total_size": stats.get("total_size", 0),
            "by_category": {item["category"]: item["count"] for item in category_stats},
            "by_status": {item["status"]: item["count"] for item in status_stats},
            "recent_uploads": [
                {
                    "filename": item["filename"],
                    "category": item["category"],
                    "size": item["file_size"],
                    "uploadedAt": str(item["created_at"])
                }
                for item in recent_uploads
            ]
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "total_files": 0,
            "total_size": 0,
            "by_category": {},
            "by_status": {},
            "recent_uploads": []
        }


@router.get("/database-details")
async def get_database_details():
    """
    è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        {
            totalFiles: int,
            totalSize: int,
            storageUsed: float (MB),
            knowledgeEntries: int,
            lastUpdate: str
        }
    """
    try:
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        total_files_result = db.query_one("SELECT COUNT(*) as count FROM uploaded_files")
        total_files = total_files_result['count'] if total_files_result else 0
        
        # ç»Ÿè®¡æ€»å¤§å°
        total_size_result = db.query_one("SELECT COALESCE(SUM(file_size), 0) as total FROM uploaded_files")
        total_size = total_size_result['total'] if total_size_result else 0
        storage_used_mb = round(total_size / (1024 * 1024), 2)
        
        # ç»Ÿè®¡çŸ¥è¯†åº“æ¡ç›®
        try:
            kb_result = db.query_one("SELECT COUNT(*) as count FROM files")
            kb_count = kb_result['count'] if kb_result else 0
        except:
            kb_count = 0
        
        # è·å–æœ€åæ›´æ–°æ—¶é—´
        last_update_result = db.query_one(
            "SELECT MAX(created_at) as last_update FROM uploaded_files"
        )
        last_update = last_update_result['last_update'] if last_update_result and last_update_result['last_update'] else "æœªçŸ¥"
        if last_update != "æœªçŸ¥":
            last_update = str(last_update)
        
        return {
            "totalFiles": total_files,
            "totalSize": total_size,
            "storageUsed": storage_used_mb,
            "knowledgeEntries": kb_count,
            "lastUpdate": last_update
        }
    except Exception as e:
        logger.error(f"Error getting database details: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®åº“è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.get("/knowledge-base-entries")
async def get_knowledge_base_entries():
    """
    è·å–çŸ¥è¯†åº“æ¡ç›®åˆ—è¡¨
    
    Returns:
        List of knowledge base entries
    """
    try:
        # ä»filesè¡¨æŸ¥è¯¢ï¼ˆä½œä¸ºçŸ¥è¯†åº“ï¼‰ï¼ŒJOIN uploaded_filesè·å–åŸå§‹æ–‡ä»¶å
        entries = db.query("""
            SELECT 
                f.id,
                COALESCE(uf.filename, f.filename) as title,
                f.doc_type as category,
                COALESCE(uf.filename, f.filename) as "fileName",
                f.created_at as "createdAt",
                COUNT(c.id) as "chapterCount"
            FROM files f
            LEFT JOIN uploaded_files uf ON f.id = uf.id
            LEFT JOIN chapters c ON f.id = c.file_id
            GROUP BY f.id, f.filename, uf.filename, f.doc_type, f.created_at
            ORDER BY f.created_at DESC
            LIMIT 100
        """)
        
        if entries:
            formatted = []
            for entry in entries:
                formatted.append({
                    "id": entry.get("id"),
                    "title": entry.get("title"),
                    "category": entry.get("category", "reference"),
                    "fileName": entry.get("fileName"),
                    "createdAt": str(entry.get("createdAt", "")),
                    "chapterCount": entry.get("chapterCount", 0)
                })
            return formatted
        
        return []
    except Exception as e:
        logger.error(f"Error getting knowledge base entries: {e}", exc_info=True)
        return []


@router.get("/knowledge-base")
async def get_knowledge_base_alias():
    """å…¼å®¹æ—§è·¯å¾„ï¼Œè¿”å›çŸ¥è¯†åº“æ¡ç›®åˆ—è¡¨"""
    return await get_knowledge_base_entries()


@router.get("/document-indexes")
async def get_document_indexes(fileId: Optional[str] = None):
    """
    è·å–æ–‡æ¡£ç´¢å¼•åˆ—è¡¨
    
    Args:
        fileId: å¯é€‰ï¼ŒæŒ‡å®šæ–‡ä»¶IDï¼ˆä½¿ç”¨é©¼å³°å‘½ååŒ¹é…å‰ç«¯ï¼‰
    
    Returns:
        List of document indexes with hierarchical structure
    """
    try:
        def build_chapter_tree(chapter_rows):
            """å°†å¹³é“ºçš„ç« èŠ‚åˆ—è¡¨æ„å»ºä¸ºæ ‘ç»“æ„"""
            roots = []
            stack = []  # ç»´æŠ¤æ¯ä¸€å±‚çš„æœ€åèŠ‚ç‚¹

            for chapter in chapter_rows:
                level = chapter.get('chapter_level') or 1
                try:
                    level_int = int(level)
                except Exception:
                    level_int = 1

                node = {
                    'number': chapter.get('chapter_number'),
                    'title': chapter.get('chapter_title'),
                    'level': level_int,
                    'pageNum': chapter.get('position_order', 1),
                    'children': []
                }

                # ç¡®ä¿æ ˆæ·±ä¸å½“å‰levelåŒ¹é…
                while stack and stack[-1]['level'] >= level_int:
                    stack.pop()

                if stack:
                    stack[-1]['children'].append(node)
                else:
                    roots.append(node)

                stack.append(node)

            return roots

        document_indexes = []
        seen_files = set()  # é˜²æ­¢é‡å¤

        # æŸ¥è¯¢æ–‡ä»¶å’Œç« èŠ‚ä¿¡æ¯ï¼ŒJOIN uploaded_files è·å–åŸå§‹æ–‡ä»¶å
        if fileId:
            files = db.query_all(
                """
                SELECT f.*, uf.filename as original_filename
                FROM files f
                LEFT JOIN uploaded_files uf ON f.id = uf.id
                WHERE f.id = %s
                """,
                (fileId,)
            )
        else:
            # æŒ‰åˆ›å»ºæ—¶é—´å€’åºè¿”å›æœ€è¿‘çš„æ–‡ä»¶
            files = db.query_all(
                """
                SELECT f.*, uf.filename as original_filename
                FROM files f
                LEFT JOIN uploaded_files uf ON f.id = uf.id
                ORDER BY f.created_at DESC
                LIMIT 50
                """
            )

        for file in files:
            file_id_str = str(file['id'])

            # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
            if file_id_str in seen_files:
                continue
            seen_files.add(file_id_str)

            # æŸ¥è¯¢ç« èŠ‚
            chapters = db.query_all(
                """
                SELECT chapter_number, chapter_title, chapter_level, position_order
                FROM chapters
                WHERE file_id = %s
                ORDER BY position_order
                """,
                (file['id'],)
            )

            # åªè¿”å›æœ‰ç« èŠ‚çš„æ–‡ä»¶
            if not chapters:
                continue

            chapter_tree = build_chapter_tree(chapters)
            
            # ä¼˜å…ˆä½¿ç”¨ JOIN æŸ¥è¯¢å¾—åˆ°çš„ original_filenameï¼ˆæ¥è‡ª uploaded_files è¡¨ï¼‰
            # å¦‚æœæ²¡æœ‰ï¼Œåˆ™å°è¯•ä» metadata ä¸­è·å–
            # æœ€åæ‰ä½¿ç”¨ files.filenameï¼ˆè¯­ä¹‰åŒ–æ–‡ä»¶åï¼‰
            display_name = file.get('original_filename') or file['filename']
            if not file.get('original_filename') and file.get('metadata') and isinstance(file['metadata'], dict):
                display_name = file['metadata'].get('original_filename', file['filename'])

            document_indexes.append({
                'id': file['id'],
                'fileName': display_name,
                'chapters': chapter_tree
            })
        
        return document_indexes
        
    except Exception as e:
        logger.error(f"Error getting document indexes: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£ç´¢å¼•å¤±è´¥: {str(e)}")


@router.get("/{file_id}")
async def get_file_detail(file_id: str):
    """
    è·å–æ–‡ä»¶è¯¦æƒ…(åŒ…å«ç« èŠ‚)
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    # è·å–æ–‡ä»¶ä¿¡æ¯
    file = db.query_one(
        "SELECT * FROM files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # è·å–ç« èŠ‚åˆ—è¡¨
    chapters = db.query(
        """
        SELECT id, chapter_number, chapter_title, chapter_level, 
               position_order, structure_data
        FROM chapters
        WHERE file_id = %s
        ORDER BY position_order
        """,
        (file_id,)
    )
    
    file['chapters'] = chapters
    return file


@router.get("/{file_id}/chapters")
async def get_chapters(file_id: str):
    """
    è·å–æ–‡ä»¶çš„æ‰€æœ‰ç« èŠ‚
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    chapters = db.query(
        "SELECT * FROM chapters WHERE file_id = %s ORDER BY position_order",
        (file_id,)
    )
    
    return {
        "file_id": file_id,
        "total": len(chapters),
        "chapters": chapters
    }


@router.get("/chapter/{chapter_id}")
async def get_chapter_detail(chapter_id: str):
    """
    è·å–ç« èŠ‚è¯¦æƒ…(åŒ…å«å®Œæ•´å†…å®¹)
    
    Args:
        chapter_id: ç« èŠ‚ID
    """
    chapter = db.query_one(
        "SELECT * FROM chapters WHERE id = %s",
        (chapter_id,)
    )
    
    if not chapter:
        raise HTTPException(status_code=404, detail="ç« èŠ‚ä¸å­˜åœ¨")
    
    return chapter


@router.delete("/{file_id}")
async def delete_file(file_id: str):
    """
    åˆ é™¤æ–‡ä»¶(åŠå…¶å…³è”çš„ç« èŠ‚)
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file = db.query_one(
        "SELECT * FROM files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ é™¤ç‰©ç†æ–‡ä»¶
    if file['filepath'] and os.path.exists(file['filepath']):
        os.remove(file['filepath'])
    
    # åˆ é™¤æ•°æ®åº“è®°å½•(CASCADEä¼šè‡ªåŠ¨åˆ é™¤å…³è”ç« èŠ‚)
    db.execute("DELETE FROM files WHERE id = %s", (file_id,))
    
    return {"status": "success", "message": "æ–‡ä»¶å·²åˆ é™¤"}


@router.delete("/uploaded/{file_id}")
async def delete_uploaded_file(file_id: str):
    """
    åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆuploaded_filesè¡¨ï¼‰
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file = db.query_one(
        "SELECT * FROM uploaded_files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ é™¤ç‰©ç†æ–‡ä»¶
    if file['file_path'] and os.path.exists(file['file_path']):
        try:
            os.remove(file['file_path'])
        except Exception as e:
            logger.warning(f"Failed to delete physical file: {e}")
    
    # åˆ é™¤æ•°æ®åº“è®°å½•
    db.execute("DELETE FROM uploaded_files WHERE id = %s", (file_id,))
    
    return {"status": "success", "message": "æ–‡ä»¶å·²åˆ é™¤"}


@router.get("/uploaded/{file_id}/download")
async def download_uploaded_file(file_id: str):
    """
    ä¸‹è½½ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆuploaded_filesè¡¨ï¼‰
    
    Args:
        file_id: æ–‡ä»¶ID
    """
    from fastapi.responses import FileResponse
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file = db.query_one(
        "SELECT * FROM uploaded_files WHERE id = %s",
        (file_id,)
    )
    
    if not file:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    file_path = file['file_path']
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ç‰©ç†è·¯å¾„ä¸å­˜åœ¨")
    
    return FileResponse(
        path=file_path,
        filename=file['filename'],
        media_type='application/octet-stream'
    )


class ProcessFilesRequest(BaseModel):
    fileIds: List[str]


@router.post("/process")
async def process_files(
    background_tasks: BackgroundTasks,
    request: ProcessFilesRequest
):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼šç”ŸæˆçŸ¥è¯†åº“å’Œæ–‡æ¡£ç´¢å¼•
    
    Args:
        fileIds: æ–‡ä»¶IDåˆ—è¡¨
    
    Returns:
        {
            status: str,
            processedFiles: int,
            documentIndexes: List[DocumentIndex]
        }
    """
    file_ids = request.fileIds
    
    if not file_ids:
        raise HTTPException(status_code=400, detail="æœªæä¾›æ–‡ä»¶ID")
    
    try:
        document_indexes = []
        
        for file_id in file_ids:
            # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
            file_info = db.query_one(
                "SELECT * FROM uploaded_files WHERE id = %s",
                (file_id,)
            )
            
            if not file_info:
                logger.warning(f"File not found: {file_id}")
                continue
            
            # è§£ææ–‡ä»¶ç”Ÿæˆæ–‡æ¡£ç´¢å¼•
            try:
                # è§£ææ–‡æ¡£ç»“æ„
                parsed_data = parse_engine.parse(file_info['file_path'], file_info['doc_type'])
                
                # ç”Ÿæˆç« èŠ‚ç´¢å¼•
                chapters = parsed_data.get('chapters', [])
                chapter_index = []
                
                for chapter in chapters:
                    chapter_index.append({
                        'title': chapter.get('chapter_title', chapter.get('title', 'æœªå‘½åç« èŠ‚')),
                        'level': chapter.get('chapter_level', chapter.get('level', 1)),
                        'pageNum': chapter.get('page_number', chapter.get('page', 1)),
                        'children': []  # å¯ä»¥é€’å½’å¤„ç†å­ç« èŠ‚
                    })
                
                document_indexes.append({
                    'id': file_id,
                    'fileName': file_info['filename'],
                    'chapters': chapter_index
                })
                
                # æå–çŸ¥è¯†åº“æ¡ç›®ï¼ˆåå°ä»»åŠ¡ï¼‰
                background_tasks.add_task(
                    extract_knowledge_entries,
                    file_id,
                    file_info['filename'],
                    parsed_data.get('content', '')
                )
                
            except Exception as parse_error:
                logger.error(f"Error parsing file {file_id}: {parse_error}")
                continue
        
        return {
            "status": "success",
            "processedFiles": len(document_indexes),
            "documentIndexes": document_indexes
        }
        
    except Exception as e:
        logger.error(f"Error processing files: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")


def extract_knowledge_entries(file_id: str, filename: str, content: str):
    """
    åå°ä»»åŠ¡ï¼šä»æ–‡æ¡£å†…å®¹ä¸­æå–çŸ¥è¯†åº“æ¡ç›®
    """
    try:
        logger.info(f"Extracting knowledge entries from: {filename}")
        
        # è¿™é‡Œå¯ä»¥è°ƒç”¨LLMæˆ–è§„åˆ™å¼•æ“æå–çŸ¥è¯†ç‚¹
        # ç®€åŒ–ç‰ˆæœ¬ï¼šæŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 50]
        
        # ç¡®ä¿knowledge_baseè¡¨å­˜åœ¨
        try:
            db.execute("""
                CREATE TABLE IF NOT EXISTS knowledge_base (
                    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
                    title text NOT NULL,
                    content text NOT NULL,
                    category text DEFAULT 'general',
                    file_id uuid,
                    file_name text,
                    created_at timestamptz DEFAULT now()
                )
            """)
        except:
            pass
        
        # æ’å…¥çŸ¥è¯†æ¡ç›®ï¼ˆç¤ºä¾‹ï¼šæå–å‰10ä¸ªæ®µè½ï¼‰
        for idx, para in enumerate(paragraphs[:10]):
            try:
                # ç”Ÿæˆæ ‡é¢˜ï¼ˆå–å‰30ä¸ªå­—ç¬¦ï¼‰
                title = para[:30] + '...' if len(para) > 30 else para
                
                db.execute(
                    """
                    INSERT INTO knowledge_base (title, content, category, file_id, file_name)
                    VALUES (%s, %s, %s, %s, %s)
                    """,
                    (title, para, 'auto-extracted', file_id, filename)
                )
            except Exception as insert_error:
                logger.warning(f"Failed to insert knowledge entry: {insert_error}")
        
        logger.info(f"Knowledge extraction completed for: {filename}")
        
    except Exception as e:
        logger.error(f"Error extracting knowledge entries: {e}")
