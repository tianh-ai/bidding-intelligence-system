# -*- coding: utf-8 -*-
"""
integrated_parser_with_tracking.py - é›†æˆéªŒè¯è„šæœ¬
æ¼”ç¤ºverify_new_parser.pyä¸init_database.pyçš„å®Œæ•´å…³ç³»

å·¥ä½œæµç¨‹:
1. ä»uploaded_filesè¡¨è¯»å–å¾…å¤„ç†çš„æ–‡ä»¶
2. è°ƒç”¨verify_new_parserè¿›è¡ŒéªŒè¯
3. å°†éªŒè¯ç»“æœå­˜å…¥parsing_resultså’Œverification_tracking
4. ä»è§£æç»“æœæå–çŸ¥è¯†æ¡é¡¹åˆ°knowledge_base
"""

import asyncio
import asyncpg
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional
from uuid import uuid4
from loguru import logger

# é…ç½®
DB_HOST = "localhost"
DB_PORT = 5432
DB_NAME = "bidding_db"
DB_USER = "postgres"
DB_PASSWORD = "postgres"

logger.add(
    "/Volumes/ssd/bidding-data/logs/integrated_parser.log",
    rotation="500 MB",
    level="INFO"
)


class VerificationResult:
    """verify_new_parser.pyçš„è¾“å‡ºæ•°æ®ç»“æ„"""
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.total_toc_items = 16
        self.matched_count = 0
        self.toc_items = []  # å‚è€ƒTOC
        self.extracted_chapters = []  # å®é™…æå–çš„ç« èŠ‚
        self.verification_details = []
        self.parsing_time = 0.0
        
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸ï¼Œç”¨äºJSONå­˜å‚¨"""
        return {
            "file_path": self.file_path,
            "total_toc_items": self.total_toc_items,
            "matched_count": self.matched_count,
            "success_rate": (self.matched_count / self.total_toc_items * 100) if self.total_toc_items > 0 else 0,
            "extracted_chapter_count": len(self.extracted_chapters),
            "verification_details": self.verification_details,
            "toc_verification": self._generate_toc_verification(),
            "timestamp": datetime.now().isoformat()
        }
    
    def _generate_toc_verification(self):
        """ç”ŸæˆTOCéªŒè¯è¯¦æƒ…"""
        return [
            {
                "toc_item": item,
                "matched": item in self.extracted_chapters,
                "extracted_chapter": item if item in self.extracted_chapters else None,
                "similarity_score": 0.95 if item in self.extracted_chapters else 0.0
            }
            for item in self.toc_items
        ]


class IntegratedParserTracker:
    """é›†æˆè§£æå’Œè¿½è¸ªç³»ç»Ÿ"""
    
    def __init__(self):
        self.db = None
        
    async def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        self.db = await asyncpg.connect(
            host=DB_HOST,
            port=DB_PORT,
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD
        )
        logger.info("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    async def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        if self.db:
            await self.db.close()
    
    async def get_pending_files(self) -> List[Dict]:
        """ä»uploaded_filesè¡¨è·å–å¾…å¤„ç†çš„æ–‡ä»¶"""
        query = """
        SELECT id, file_name, file_path, file_size, created_at
        FROM uploaded_files
        WHERE parse_status = 'pending'
        LIMIT 10
        """
        rows = await self.db.fetch(query)
        return [dict(row) for row in rows]
    
    async def simulate_verify_parser(self, file_path: str) -> VerificationResult:
        """æ¨¡æ‹Ÿverify_new_parser.pyçš„æ‰§è¡Œ
        
        åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„ParseEngineå’ŒEnhancedChapterExtractor
        """
        logger.info(f"ğŸ” å¼€å§‹éªŒè¯: {file_path}")
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        await asyncio.sleep(0.5)
        
        # æ¨¡æ‹ŸéªŒè¯ç»“æœ
        result = VerificationResult(file_path)
        result.toc_items = [
            "ç¬¬ä¸€éƒ¨åˆ†  æŠ•æ ‡é‚€è¯·",
            "ä¸€ã€æŠ•æ ‡è¯´æ˜",
            "äºŒã€æŠ•æ ‡äººèµ„æ ¼è¦æ±‚",
            "ä¸‰ã€æŠ•æ ‡äººåº”å…·å¤‡çš„æ¡ä»¶",
            "å››ã€æ‹›æ ‡äººè”ç³»æ–¹å¼",
            "ç¬¬äºŒéƒ¨åˆ†  æŠ•æ ‡äººé¡»çŸ¥",
            "äº”ã€æŠ•æ ‡å‰çš„å‡†å¤‡",
            "å…­ã€æŠ•æ ‡æ–‡ä»¶çš„æ„æˆ",
            "ä¸ƒã€æŠ•æ ‡æ–‡ä»¶çš„åˆ¶ä½œ",
            "å…«ã€æŠ•æ ‡æ–‡ä»¶çš„é€äº¤",
            "ç¬¬ä¸‰éƒ¨åˆ†  æŠ€æœ¯è§„æ ¼",
            "ä¹ã€æŠ€æœ¯è¦æ±‚",
            "åã€æ€§èƒ½æŒ‡æ ‡",
            "åä¸€ã€è´¨é‡æ ‡å‡†",
            "åäºŒã€éªŒæ”¶æ¡ä»¶",
            "åä¸‰ã€æœåŠ¡è¦æ±‚"
        ]
        
        # æ¨¡æ‹Ÿæå–çš„ç« èŠ‚ (14/16åŒ¹é…)
        result.extracted_chapters = result.toc_items[:-2]  # ç¼ºå°‘æœ€åä¸¤é¡¹
        result.matched_count = len(result.extracted_chapters)
        result.parsing_time = 3.45
        
        logger.info(f"âœ“ éªŒè¯å®Œæˆ: {result.matched_count}/{result.total_toc_items} åŒ¹é… (æˆåŠŸç‡: {result.matched_count/result.total_toc_items*100:.1f}%)")
        
        return result
    
    async def save_verification_result(self, file_id: str, file_name: str, 
                                      verify_result: VerificationResult) -> str:
        """ä¿å­˜éªŒè¯ç»“æœåˆ°æ•°æ®åº“
        
        è¿™ä¸ªå‡½æ•°æ¼”ç¤ºäº†verify_new_parser.pyè¾“å‡ºä¸init_database.pyè¡¨ç»“æ„çš„å…³ç³»
        
        æµç¨‹:
        1. æ›´æ–°uploaded_filesçš„parse_status
        2. åˆ›å»ºparsing_resultsè®°å½•
        3. åˆ›å»ºverification_trackingè®°å½•
        4. åˆ›å»ºmappingå…³ç³»
        5. ä»éªŒè¯ç»“æœæå–çŸ¥è¯†æ¡é¡¹åˆ°knowledge_base
        """
        start_time = time.time()
        verification_result_id = str(uuid4())
        parsing_result_id = str(uuid4())
        
        try:
            # Step 1: æ›´æ–°uploaded_filesçš„parse_status
            logger.info(f"ğŸ“ Step 1: æ›´æ–° uploaded_files (file_id: {file_id})")
            update_query = """
            UPDATE uploaded_files
            SET parse_status = 'processing', updated_at = NOW()
            WHERE id = $1
            """
            await self.db.execute(update_query, file_id)
            
            # Step 2: åˆ›å»ºparsing_resultsè®°å½•
            logger.info(f"ğŸ“ Step 2: åˆ›å»º parsing_results è®°å½•")
            parsing_query = """
            INSERT INTO parsing_results 
            (id, file_id, chapter_count, parsing_time, parsing_status, 
             result_json, accuracy_score, matched_toc_items, total_toc_items)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
            """
            
            result_dict = verify_result.to_dict()
            accuracy = (verify_result.matched_count / verify_result.total_toc_items * 100)
            
            await self.db.execute(
                parsing_query,
                parsing_result_id,  # id
                file_id,  # file_id (FK)
                len(verify_result.extracted_chapters),  # chapter_count
                verify_result.parsing_time,  # parsing_time
                "completed",  # parsing_status
                result_dict,  # result_json (å®Œæ•´éªŒè¯ç»“æœ)
                accuracy,  # accuracy_score
                verify_result.matched_count,  # matched_toc_items
                verify_result.total_toc_items  # total_toc_items
            )
            logger.info(f"âœ“ parsing_results created: {parsing_result_id}")
            
            # Step 3: åˆ›å»ºverification_trackingè®°å½•
            logger.info(f"ğŸ“ Step 3: åˆ›å»º verification_tracking è®°å½•")
            verify_tracking_query = """
            INSERT INTO verification_tracking
            (id, file_id, parsing_result_id, verification_status,
             verification_start_time, verification_end_time,
             total_toc_items, matched_toc_items, success_rate,
             extracted_chapter_count, toc_verification_details)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
            """
            
            await self.db.execute(
                verify_tracking_query,
                verification_result_id,  # id
                file_id,  # file_id (FK)
                parsing_result_id,  # parsing_result_id (FK)
                "completed",  # verification_status
                datetime.now(),  # verification_start_time
                datetime.now(),  # verification_end_time
                verify_result.total_toc_items,  # total_toc_items
                verify_result.matched_count,  # matched_toc_items
                accuracy,  # success_rate
                len(verify_result.extracted_chapters),  # extracted_chapter_count
                result_dict.get("toc_verification")  # toc_verification_details
            )
            logger.info(f"âœ“ verification_tracking created: {verification_result_id}")
            
            # Step 4: åˆ›å»ºmappingå…³ç³»
            logger.info(f"ğŸ“ Step 4: åˆ›å»º parsing_verification_mapping å…³ç³»")
            mapping_query = """
            INSERT INTO parsing_verification_mapping
            (file_id, parsing_result_id, verification_tracking_id,
             overall_quality_score)
            VALUES ($1, $2, $3, $4)
            """
            
            await self.db.execute(
                mapping_query,
                file_id,  # file_id
                parsing_result_id,  # parsing_result_id
                verification_result_id,  # verification_tracking_id
                accuracy / 100.0  # overall_quality_score (0-1)
            )
            logger.info("âœ“ mapping created")
            
            # Step 5: ä»éªŒè¯ç»“æœæå–çŸ¥è¯†æ¡é¡¹åˆ°knowledge_base
            logger.info(f"ğŸ“ Step 5: æå–çŸ¥è¯†æ¡é¡¹åˆ° knowledge_base")
            
            knowledge_count = 0
            for i, chapter in enumerate(verify_result.extracted_chapters):
                # åªæ’å…¥åŒ¹é…çš„ç« èŠ‚
                kb_query = """
                INSERT INTO knowledge_base
                (file_id, title, content, category, file_name,
                 chapter_source, extraction_confidence)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                """
                
                await self.db.execute(
                    kb_query,
                    file_id,  # file_id (FK)
                    f"{chapter}",  # title
                    f"å†…å®¹è¯¦æƒ…: {chapter}",  # content
                    "æ‹›æ ‡æ¡æ¬¾",  # category
                    file_name,  # file_name
                    chapter,  # chapter_source
                    0.95  # extraction_confidence (é«˜ç½®ä¿¡åº¦)
                )
                knowledge_count += 1
            
            logger.info(f"âœ“ {knowledge_count} æ¡çŸ¥è¯†æ¡é¡¹å·²åˆ›å»º")
            
            # Step 6: æœ€åæ›´æ–°uploaded_filesçš„parse_statusä¸ºcompleted
            logger.info(f"ğŸ“ Step 6: å®Œæˆå¤„ç†ï¼Œæ›´æ–° uploaded_files")
            final_update_query = """
            UPDATE uploaded_files
            SET parse_status = 'completed', updated_at = NOW()
            WHERE id = $1
            """
            await self.db.execute(final_update_query, file_id)
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            return parsing_result_id
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            # å›æ»š: æ›´æ–°parse_statusä¸ºfailed
            await self.db.execute(
                "UPDATE uploaded_files SET parse_status = 'failed' WHERE id = $1",
                file_id
            )
            raise
    
    async def query_file_lifecycle(self, file_id: str):
        """æŸ¥è¯¢å•ä¸ªæ–‡ä»¶çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ"""
        query = """
        SELECT 
            uf.id as file_id,
            uf.file_name,
            uf.upload_status,
            uf.parse_status,
            uf.created_at as upload_time,
            uf.updated_at as last_updated,
            
            pr.id as parsing_result_id,
            pr.chapter_count,
            pr.parsing_time,
            pr.accuracy_score,
            pr.matched_toc_items,
            pr.total_toc_items,
            
            vt.id as verification_tracking_id,
            vt.verification_status,
            vt.success_rate,
            
            COUNT(DISTINCT kb.id) as knowledge_items_count
            
        FROM uploaded_files uf
        LEFT JOIN parsing_results pr ON uf.id = pr.file_id
        LEFT JOIN verification_tracking vt ON pr.id = vt.parsing_result_id
        LEFT JOIN knowledge_base kb ON uf.id = kb.file_id
        
        WHERE uf.id = $1
        GROUP BY uf.id, pr.id, vt.id
        """
        
        result = await self.db.fetchval(query, file_id)
        return dict(result) if result else None


async def demonstrate_relationship():
    """æ¼”ç¤ºverify_new_parser.pyä¸init_database.pyçš„å®Œæ•´å…³ç³»"""
    
    tracker = IntegratedParserTracker()
    await tracker.connect()
    
    try:
        print("="*70)
        print("ğŸ“Š verify_new_parser.py â†” init_database.py å…³ç³»æ¼”ç¤º")
        print("="*70)
        print()
        
        # è·å–å¾…å¤„ç†æ–‡ä»¶
        pending_files = await tracker.get_pending_files()
        
        if not pending_files:
            logger.info("âš ï¸ æ²¡æœ‰å¾…å¤„ç†çš„æ–‡ä»¶")
            print("æç¤º: è¯·å…ˆä½¿ç”¨æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½å°†PDFæ–‡ä»¶ä¸Šä¼ åˆ°ç³»ç»Ÿ")
            print("      æˆ–ä½¿ç”¨æµ‹è¯•è„šæœ¬åˆ›å»ºæµ‹è¯•æ•°æ®")
            return
        
        logger.info(f"å‘ç° {len(pending_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        for file_info in pending_files[:1]:  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºæ¼”ç¤º
            file_id = file_info['id']
            file_name = file_info['file_name']
            file_path = file_info['file_path']
            
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {file_name}")
            print(f"   Path: {file_path}")
            print(f"   FileID: {file_id}")
            print()
            
            # 1. æ‰§è¡Œverify_new_parseréªŒè¯
            verify_result = await tracker.simulate_verify_parser(file_path)
            
            # 2. ä¿å­˜éªŒè¯ç»“æœåˆ°æ•°æ®åº“ (æ¼”ç¤ºå®Œæ•´çš„å…³ç³»æµç¨‹)
            print()
            print("ğŸ’¾ ä¿å­˜éªŒè¯ç»“æœåˆ°æ•°æ®åº“...")
            parsing_result_id = await tracker.save_verification_result(
                file_id, file_name, verify_result
            )
            
            # 3. æŸ¥è¯¢æ–‡ä»¶çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
            print()
            print("ğŸ“Š æŸ¥è¯¢æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸ...")
            lifecycle = await tracker.query_file_lifecycle(file_id)
            
            if lifecycle:
                print(f"\næ–‡ä»¶å¤„ç†æµç¨‹:")
                print(f"  ä¸Šä¼ æ—¶é—´: {lifecycle['upload_time']}")
                print(f"  ä¸Šä¼ çŠ¶æ€: {lifecycle['upload_status']}")
                print(f"  è§£æçŠ¶æ€: {lifecycle['parse_status']}")
                print(f"  æœ€åæ›´æ–°: {lifecycle['last_updated']}")
                print(f"\nè§£æç»“æœ:")
                print(f"  è§£æè€—æ—¶: {lifecycle['parsing_time']}ç§’")
                print(f"  ç« èŠ‚æ•°: {lifecycle['chapter_count']}")
                print(f"  å‡†ç¡®ç‡: {lifecycle['accuracy_score']:.1f}% ({lifecycle['matched_toc_items']}/{lifecycle['total_toc_items']})")
                print(f"\nè¿½è¸ªä¿¡æ¯:")
                print(f"  éªŒè¯çŠ¶æ€: {lifecycle['verification_status']}")
                print(f"  çŸ¥è¯†æ¡é¡¹: {lifecycle['knowledge_items_count']}æ¡")
        
        print()
        print("="*70)
        print("âœ… å…³ç³»æ¼”ç¤ºå®Œæˆ")
        print("="*70)
        
    finally:
        await tracker.disconnect()


if __name__ == "__main__":
    asyncio.run(demonstrate_relationship())
