"""
å®Œæ•´åŠŸèƒ½æµ‹è¯• - éªŒè¯æ‰€æœ‰æ–°å®ç°çš„å¼•æ“
åŒ…æ‹¬ç”Ÿæˆã€è¯„åˆ†ã€å¯¹æ¯”ã€å¼ºåŒ–å­¦ä¹ åé¦ˆ
ï¼ˆç›´æ¥å¯¼å…¥æ–°å¼•æ“æ¨¡å—ï¼Œé¿å…æ•°æ®åº“è¿æ¥ï¼‰
"""

import sys
import asyncio
from pathlib import Path
import importlib.util

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = Path(__file__).parent
sys.path.insert(0, str(backend_dir))

print("ğŸš€ å¼€å§‹å®Œæ•´åŠŸèƒ½æµ‹è¯•...\n")

tests_passed = 0
tests_total = 0

# è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
try:
    loop = asyncio.get_event_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# ==================== æµ‹è¯•1: GenerationEngine ====================
print("="*60)
print("æµ‹è¯•1: ç”Ÿæˆå¼•æ“ (GenerationEngine)")
print("="*60)
tests_total += 1
try:
    # ç›´æ¥å¯¼å…¥æ–‡ä»¶ï¼Œç»•è¿‡ engines/__init__.py ä¸­çš„ä¾èµ–
    spec = importlib.util.spec_from_file_location(
        "generation_engine",
        backend_dir / "engines" / "generation_engine.py"
    )
    gen_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(gen_module)
    
    GenerationEngine = gen_module.GenerationEngine
    GenerationStrategy = gen_module.GenerationStrategy
    GenerationMode = gen_module.GenerationMode
    
    gen_engine = GenerationEngine()
    print("âœ… å¯¼å…¥æˆåŠŸ: GenerationEngine")
    
    # æ¨¡æ‹Ÿç”ŸæˆæŠ•æ ‡ä¹¦
    async def test_generation():
        version = await gen_engine.generate_proposal(
            tender_id="tender_001",
            template_id="template_001",
            strategy=GenerationStrategy.BALANCED,
            mode=GenerationMode.FULL
        )
        assert version.overall_score > 0, "ç”Ÿæˆè¯„åˆ†åº”å¤§äº0"
        assert len(version.contents) > 0, "åº”ç”Ÿæˆå†…å®¹"
        return version
    
    version = loop.run_until_complete(test_generation())
    
    print(f"   - ç”Ÿæˆç‰ˆæœ¬: {version.version_id}")
    print(f"   - æ€»ä½“è¯„åˆ†: {version.overall_score:.1f}")
    print(f"   - ç”Ÿæˆå†…å®¹æ•°: {len(version.contents)}")
    print(f"   - ç”Ÿæˆç­–ç•¥: {version.strategy.value}")
    print(f"   - ç”Ÿæˆæ¨¡å¼: {version.mode.value}")
    
    tests_passed += 1
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# ==================== æµ‹è¯•2: ScoringEngine ====================
print("\n" + "="*60)
print("æµ‹è¯•2: è¯„åˆ†å¼•æ“ (ScoringEngine)")
print("="*60)
tests_total += 1
try:
    spec = importlib.util.spec_from_file_location(
        "scoring_engine",
        backend_dir / "engines" / "scoring_engine.py"
    )
    score_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(score_module)
    
    ScoringEngine = score_module.ScoringEngine
    
    score_engine = ScoringEngine()
    print("âœ… å¯¼å…¥æˆåŠŸ: ScoringEngine")
    
    # æŸ¥çœ‹è¯„åˆ†æ ‡å‡†
    criteria_count = len(score_engine.scoring_criteria)
    print(f"   - è¯„åˆ†æ ‡å‡†æ•°: {criteria_count}")
    
    # æŒ‰ç»´åº¦ç»Ÿè®¡
    dimensions = {}
    for criteria in score_engine.scoring_criteria:
        dim = criteria.dimension.value
        dimensions[dim] = dimensions.get(dim, 0) + 1
    
    print(f"   - è¯„åˆ†ç»´åº¦: {list(dimensions.keys())}")
    print(f"   - ç»´åº¦åˆ†å¸ƒ: {dimensions}")
    
    # æ¨¡æ‹Ÿè¯„åˆ†
    async def test_scoring():
        proposal_content = {
            "quality_score": 75,
            "relevance_score": 80,
            "completeness_score": 85,
            "metric_comp_001": True,
            "metric_comp_002": True,
            "metric_tech_003": True
        }
        
        score = await score_engine.score_proposal(
            proposal_id="prop_001",
            tender_id="tender_001",
            proposal_content=proposal_content
        )
        
        assert score.overall_score > 0, "è¯„åˆ†åº”å¤§äº0"
        assert len(score.dimension_scores) > 0, "åº”æœ‰ç»´åº¦è¯„åˆ†"
        return score
    
    proposal_score = loop.run_until_complete(test_scoring())
    
    print(f"   - æ€»ä½“è¯„åˆ†: {proposal_score.overall_score:.1f}")
    print(f"   - ç¡¬æŒ‡æ ‡é€šè¿‡: {proposal_score.hard_metric_pass}")
    print(f"   - ç»´åº¦è¯„åˆ†æ•°: {len(proposal_score.dimension_scores)}")
    
    for dim_score in proposal_score.dimension_scores[:3]:
        print(f"   - {dim_score.dimension.value}: {dim_score.score:.1f}")
    
    tests_passed += 1
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# ==================== æµ‹è¯•3: ComparisonEngine ====================
print("\n" + "="*60)
print("æµ‹è¯•3: å¯¹æ¯”å¼•æ“ (ComparisonEngine)")
print("="*60)
tests_total += 1
try:
    spec = importlib.util.spec_from_file_location(
        "comparison_engine",
        backend_dir / "engines" / "comparison_engine.py"
    )
    comp_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(comp_module)
    
    ComparisonEngine = comp_module.ComparisonEngine
    
    comp_engine = ComparisonEngine()
    print("âœ… å¯¼å…¥æˆåŠŸ: ComparisonEngine")
    
    # æ¨¡æ‹Ÿå¯¹æ¯”
    async def test_comparison():
        doc1_content = {
            "sections": {
                "sec_1": {
                    "name": "é¡¹ç›®æ¦‚è¿°",
                    "content": "æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå¤§å‹æŠ•æ ‡é¡¹ç›®ï¼Œæ¶‰åŠå¤šä¸ªä¸“ä¸šé¢†åŸŸå’Œå¤æ‚çš„æŠ€æœ¯è¦æ±‚ã€‚"
                },
                "sec_2": {
                    "name": "æŠ€æœ¯æ–¹æ¡ˆ",
                    "content": "æˆ‘ä»¬æå‡ºçš„æŠ€æœ¯æ–¹æ¡ˆé‡‡ç”¨æœ€å…ˆè¿›çš„æ¶æ„è®¾è®¡ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ‰©å±•æ€§ã€‚"
                }
            }
        }
        
        doc2_content = {
            "sections": {
                "sec_1": {
                    "name": "é¡¹ç›®æ¦‚è¿°",
                    "content": "æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå¤§å‹æŠ•æ ‡é¡¹ç›®ï¼Œæ¶‰åŠå¤šä¸ªä¸“ä¸šé¢†åŸŸå’Œå¤æ‚çš„æŠ€æœ¯è¦æ±‚ã€‚ä¼˜åŒ–äº†æ–¹æ¡ˆã€‚"
                },
                "sec_2": {
                    "name": "æŠ€æœ¯æ–¹æ¡ˆ",
                    "content": "æˆ‘ä»¬æå‡ºçš„æ”¹è¿›æŠ€æœ¯æ–¹æ¡ˆé‡‡ç”¨æœ€å…ˆè¿›çš„æ¶æ„è®¾è®¡ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§ã€å¯é æ€§å’Œæ‰©å±•æ€§ã€‚"
                },
                "sec_3": {
                    "name": "å®æ–½è®¡åˆ’",
                    "content": "é¡¹ç›®åˆ†ä¸‰é˜¶æ®µå®æ–½ï¼Œæ€»è®¡12ä¸ªæœˆå®Œæˆã€‚"
                }
            }
        }
        
        comparison = await comp_engine.compare_documents(
            doc1_id="doc_001",
            doc1_content=doc1_content,
            doc2_id="doc_002",
            doc2_content=doc2_content
        )
        
        assert comparison.overall_similarity >= 0, "ç›¸ä¼¼åº¦åº”æœ‰æ•ˆ"
        return comparison
    
    comparison = loop.run_until_complete(test_comparison())
    
    print(f"   - æ€»ä½“ç›¸ä¼¼åº¦: {comparison.overall_similarity:.1f}%")
    print(f"   - ç›¸ä¼¼åº¦ç­‰çº§: {comparison.similarity_level.value}")
    print(f"   - æ€»å·®å¼‚æ•°: {comparison.total_differences}")
    print(f"   - ç« èŠ‚å¯¹æ¯”æ•°: {len(comparison.section_comparisons)}")
    print(f"   - å­—æ•°å˜åŒ–: {comparison.total_word_count_change:+d}")
    
    tests_passed += 1
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# ==================== æµ‹è¯•4: ReinforcementLearningFeedback ====================
print("\n" + "="*60)
print("æµ‹è¯•4: å¼ºåŒ–å­¦ä¹ åé¦ˆæœºåˆ¶ (ReinforcementLearningFeedback)")
print("="*60)
tests_total += 1
try:
    spec = importlib.util.spec_from_file_location(
        "reinforcement_feedback",
        backend_dir / "engines" / "reinforcement_feedback.py"
    )
    feedback_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(feedback_module)
    
    ReinforcementLearningFeedback = feedback_module.ReinforcementLearningFeedback
    FeedbackType = feedback_module.FeedbackType
    ErrorSeverity = feedback_module.ErrorSeverity
    
    feedback_engine = ReinforcementLearningFeedback()
    print("âœ… å¯¼å…¥æˆåŠŸ: ReinforcementLearningFeedback")
    
    # è®°å½•é”™è¯¯
    async def test_feedback():
        # è®°å½•é”™è¯¯
        error = await feedback_engine.record_error(
            proposal_id="prop_001",
            error_type="format_error",
            severity=ErrorSeverity.MINOR,
            description="è¡¨æ ¼æ ¼å¼ä¸ç¬¦åˆè¦æ±‚",
            location="ç¬¬2ç« ç¬¬1è¡¨"
        )
        assert error.error_id, "åº”ç”Ÿæˆé”™è¯¯ID"
        
        # æäº¤åé¦ˆ
        feedback = await feedback_engine.submit_feedback(
            proposal_id="prop_001",
            feedback_type=FeedbackType.CORRECTIVE,
            score=72,
            content="éœ€è¦æ”¹è¿›è¡¨æ ¼æ ¼å¼å’Œå†…å®¹ç»“æ„"
        )
        assert feedback.feedback_id, "åº”ç”Ÿæˆåé¦ˆID"
        
        # åˆ†ææ¨¡å¼
        patterns = await feedback_engine.analyze_patterns()
        
        # è·å–æ”¹è¿›å»ºè®®
        recommendations = await feedback_engine.get_improvement_recommendations(days=7)
        
        # è·å–æ¨¡å‹æŒ‡æ ‡
        metrics = await feedback_engine.get_model_performance_metrics()
        
        return error, feedback, patterns, recommendations, metrics
    
    error, feedback, patterns, recommendations, metrics = loop.run_until_complete(test_feedback())
    
    print(f"   - è®°å½•é”™è¯¯: {error.error_id}")
    print(f"   - é”™è¯¯ç±»å‹: {error.error_type}")
    print(f"   - é”™è¯¯ä¸¥é‡åº¦: {error.severity.value}")
    print(f"   - æäº¤åé¦ˆ: {feedback.feedback_id}")
    print(f"   - åé¦ˆè¯„åˆ†: {feedback.score}")
    print(f"   - å‘ç°æ¨¡å¼æ•°: {len(patterns)}")
    print(f"   - æ€»é”™è¯¯æ•°: {metrics['total_error_records']}")
    print(f"   - æ€»åé¦ˆæ•°: {metrics['total_feedback_records']}")
    print(f"   - æ”¹è¿›å»ºè®®æ•°: {len(recommendations.get('suggestions', []))}")
    
    tests_passed += 1
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# ==================== æµ‹è¯•5: æ£€æŸ¥æ–°APIè·¯ç”±æ–‡ä»¶ ====================
print("\n" + "="*60)
print("æµ‹è¯•5: æ–°å¢APIè·¯ç”±æ–‡ä»¶éªŒè¯")
print("="*60)
tests_total += 1
try:
    # ç›´æ¥è¯»å–è·¯ç”±æ–‡ä»¶æ£€æŸ¥ç«¯ç‚¹å®šä¹‰ï¼ˆä¸å¯¼å…¥ä»¥é¿å…æ•°æ®åº“è¿æ¥ï¼‰
    router_file = backend_dir / "routers" / "enhanced.py"
    assert router_file.exists(), "enhanced.py è·¯ç”±æ–‡ä»¶ä¸å­˜åœ¨"
    
    with open(router_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å®šä¹‰äº†æ‰€æœ‰ç«¯ç‚¹
    endpoints = [
        "@router.post(\"/generate/proposal\")",
        "@router.get(\"/generate/history/",
        "@router.post(\"/generate/compare\")",
        "@router.post(\"/score/proposal\")",
        "@router.post(\"/score/compare\")",
        "@router.get(\"/score/report/",
        "@router.post(\"/compare/documents\")",
        "@router.get(\"/compare/summary/",
        "@router.get(\"/compare/history\")",
        "@router.post(\"/feedback/error\")",
        "@router.post(\"/feedback/submit\")",
        "@router.post(\"/feedback/analyze-patterns\")",
        "@router.get(\"/feedback/recommendations\")",
        "@router.post(\"/feedback/apply-improvement\")",
        "@router.get(\"/feedback/metrics\")"
    ]
    
    found_endpoints = 0
    for endpoint in endpoints:
        if endpoint in content:
            found_endpoints += 1
    
    print("âœ… éªŒè¯æˆåŠŸ: enhanced.py è·¯ç”±æ–‡ä»¶")
    print(f"   - æ‰¾åˆ°ç«¯ç‚¹æ•°: {found_endpoints}/{len(endpoints)}")
    print(f"   - ç”Ÿæˆç›¸å…³: /generate/proposal, /generate/history, /generate/compare")
    print(f"   - è¯„åˆ†ç›¸å…³: /score/proposal, /score/compare, /score/report")
    print(f"   - å¯¹æ¯”ç›¸å…³: /compare/documents, /compare/summary, /compare/history")
    print(f"   - åé¦ˆç›¸å…³: /feedback/error, /feedback/submit, /feedback/analyze-patterns, /feedback/recommendations, /feedback/apply-improvement, /feedback/metrics")
    
    tests_passed += 1
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# ==================== æ€»ç»“ ====================
print("\n" + "="*60)
print("ğŸ“Š å®Œæ•´åŠŸèƒ½æµ‹è¯•æ€»ç»“")
print("="*60)
print(f"é€šè¿‡æµ‹è¯•: {tests_passed}/{tests_total}")
print(f"æˆåŠŸç‡: {tests_passed/tests_total*100:.1f}%")

if tests_passed == tests_total:
    print("\n" + "ğŸ‰"*30)
    print("æ­å–œï¼æ‰€æœ‰åŠŸèƒ½æµ‹è¯•100%é€šè¿‡ï¼")
    print("ğŸ‰"*30)
    print("\nâœ… æ–°åŠŸèƒ½æˆå°±éªŒè¯:")
    print("  âœ… GenerationEngine: æ™ºèƒ½æŠ•æ ‡ä¹¦ç”Ÿæˆ")
    print("  âœ… ScoringEngine: å¤šç»´åº¦è‡ªåŠ¨è¯„åˆ†")
    print("  âœ… ComparisonEngine: æ–‡æ¡£å¯¹æ¯”åˆ†æ")
    print("  âœ… ReinforcementLearningFeedback: å¼ºåŒ–å­¦ä¹ åé¦ˆ")
    print("\nâœ… åŠŸèƒ½å®Œæˆåº¦:")
    print("  âœ… ç”ŸæˆåŠŸèƒ½: æ”¯æŒä¸‰ç§ç­–ç•¥ï¼ˆä¿å®ˆ/å¹³è¡¡/åˆ›æ„ï¼‰")
    print("  âœ… ç”Ÿæˆæ¨¡å¼: å…¨æ–‡/éƒ¨åˆ†/å¢é‡ç”Ÿæˆ")
    print("  âœ… è¯„åˆ†ç»´åº¦: æŠ€æœ¯/å•†åŠ¡/åˆè§„/åˆ›æ–°/å‘ˆç° 5ç»´")
    print("  âœ… å¯¹æ¯”åŠŸèƒ½: ç« èŠ‚çº§å¯¹æ¯”ã€çƒ­åŠ›å›¾ã€ç›¸ä¼¼åº¦åˆ†æ")
    print("  âœ… åé¦ˆæœºåˆ¶: é”™è¯¯åº“ã€æ¨¡å¼è¯†åˆ«ã€ä¼˜åŒ–å»ºè®®")
    print("\nğŸš€ ç³»ç»ŸçŠ¶æ€: æ‰€æœ‰æ¨¡å—å®Œå…¨å®ç°ï¼")
    sys.exit(0)
else:
    print(f"\nâš ï¸  è¿˜æœ‰ {tests_total - tests_passed} ä¸ªæµ‹è¯•å¤±è´¥")
    sys.exit(1)
